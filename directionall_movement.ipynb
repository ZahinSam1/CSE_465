{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Dict, Box\n",
    "\n",
    "# from Agents.agent import Agent\n",
    "from Constants.constants import WHITE, RED, BLUE, SCREEN_WIDTH, SCREEN_HEIGHT\n",
    "\n",
    "from Walls.collision_detection import detect_collision\n",
    "from Walls.wall_class import Walls\n",
    "from Walls.Point_Ray import is_ray_blocked\n",
    "\n",
    "from Entities.turret import Turret\n",
    "\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVEL_5_WALLS = {\n",
    "    \"1\": {\"x\": 150, \"y\": 120, \"width\": 100, \"height\": 30},\n",
    "    \"2\": {\"x\": 300, \"y\": 80,  \"width\": 30,  \"height\": 60},\n",
    "    \"3\": {\"x\": 450, \"y\": 120, \"width\": 100, \"height\": 30},\n",
    "    \"4\": {\"x\": 600, \"y\": 180, \"width\": 30,  \"height\": 100},\n",
    "    \"5\": {\"x\": 500, \"y\": 400, \"width\": 100, \"height\": 30},\n",
    "    \"6\": {\"x\": 150, \"y\": 250, \"width\": 30,  \"height\": 200},\n",
    "    \"7\": {\"x\": 300, \"y\": 500, \"width\": 150, \"height\": 30},\n",
    "    \"far-left\": {\"x\": 0, \"y\": 0, \"width\": 30, \"height\": SCREEN_HEIGHT},\n",
    "    \"far-right\": {\"x\": SCREEN_WIDTH - 30, \"y\": 0, \"width\": 30, \"height\": SCREEN_HEIGHT},\n",
    "    \"start_top\": {\"x\": 0, \"y\": 0, \"width\": SCREEN_WIDTH, \"height\": 30},\n",
    "    \"finish_bottom\": {\"x\": 0, \"y\": SCREEN_HEIGHT - 30, \"width\": SCREEN_WIDTH, \"height\": 30},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastRay:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_cast_ray_angles(self):\n",
    "        start_angle = 0  # 65 degrees to the left\n",
    "        end_angle = 350  # 65 degrees to the right\n",
    "        angle_step = 10  # One ray every 10 degrees\n",
    "        ray_angles = np.arange(start_angle, end_angle + angle_step, angle_step).tolist()\n",
    "        ray_angles = [angle%360 for angle in ray_angles]\n",
    "\n",
    "        return ray_angles\n",
    "\n",
    "    def cast_rays(self, agent, wall_list):\n",
    "        \n",
    "        ray_angles = self.get_cast_ray_angles()\n",
    "        ray_lengths = []\n",
    "        \n",
    "        for ray_angle in ray_angles:\n",
    "            x1, y1 = agent.center\n",
    "            x2, y2 = x1 + 1000 * math.cos(math.radians(ray_angle)), y1 + 1000 * math.sin(math.radians(ray_angle))\n",
    "            lengths = None\n",
    "\n",
    "            for wall in wall_list:\n",
    "                x3, y3 = wall.x, wall.y\n",
    "                x4, y4 = wall.topright[0], wall.bottomright[1]\n",
    "\n",
    "                for side in [(x3, y3, x4, y3), (x4, y3, x4, y4), (x4, y4, x3, y4), (x3, y4, x3, y3)]:\n",
    "                    x5, y5, x6, y6 = side\n",
    "\n",
    "                    denominator = (x1 - x2) * (y5 - y6) - (y1 - y2) * (x5 - x6)\n",
    "\n",
    "                    if denominator == 0:\n",
    "                        continue\n",
    "\n",
    "                    t = ((x1 - x5) * (y5 - y6) - (y1 - y5) * (x5 - x6)) / denominator\n",
    "                    u = -((x1 - x2) * (y1 - y5) - (y1 - y2) * (x1 - x5)) / denominator\n",
    "\n",
    "                    epsilon = 1e-5  # Small epsilon value\n",
    "\n",
    "                    if 0 <= t <= 1 and 0 <= u <= 1:\n",
    "                        intersection_x = x1 + t * (x2 - x1)\n",
    "                        intersection_y = y1 + t * (y2 - y1)\n",
    "\n",
    "                        # Calculate the distance from the ray start to the intersection point\n",
    "                        distance = math.sqrt((intersection_x - x1) ** 2 + (intersection_y - y1) ** 2)\n",
    "\n",
    "                        if lengths is None or distance < lengths:\n",
    "                            lengths = distance\n",
    "                \n",
    "            if lengths is None:\n",
    "                lengths = 1000\n",
    "                \n",
    "            ray_lengths.append(lengths)\n",
    "        return ray_lengths, ray_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, agent_name, agent_index):\n",
    "        # identity\n",
    "        self.index = agent_index\n",
    "        self.agent = agent_name\n",
    "\n",
    "        # additional attributes\n",
    "        self.health = None\n",
    "        self.isHit = False\n",
    "        self.move = True\n",
    "        self.movement_speed = 300\n",
    "\n",
    "        # positional attributes\n",
    "        self.previous_position = np.array([0, 0], dtype=np.float32)\n",
    "        self.current_position = None\n",
    "        self.same_position = False\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.action = None\n",
    "        self.has_grabbed = False\n",
    "\n",
    "        # these are for the angular motion of the agent\n",
    "        self.angle = 0\n",
    "        self.center = 0\n",
    "        self.direction = 0\n",
    "        self.direction_end = 0\n",
    "        self.radius = 15\n",
    "\n",
    "        # this is custom only for the render function\n",
    "        self.draw_direction_end = 0\n",
    "\n",
    "    # for handling what the action does\n",
    "    def agent_action(self, action):\n",
    "        pass\n",
    "\n",
    "    def _get_min_left(self, walls):\n",
    "        min_x = 1000\n",
    "        for wall in walls:\n",
    "            if wall.left < min_x:\n",
    "                min_x = wall.left\n",
    "        return min_x\n",
    "\n",
    "    # for handling all the initial states\n",
    "    def agent_reset(self, width, height):\n",
    "        padding = 30\n",
    "        # updating the initial random position of the agent at 1st\n",
    "        # self.current_position = np.array(\n",
    "        #     [np.random.uniform(30, self._get_min_left(walls)), np.random.uniform(30, height - padding)],\n",
    "        #     dtype=np.float32)\n",
    "\n",
    "        # self.current_position = np.array([40, height/2], dtype=np.float32)\n",
    "\n",
    "        self.current_position = np.array([80, height-80], dtype=np.float32)\n",
    "\n",
    "        # updating the initial orientation to 0 degree at 1st\n",
    "        theta = math.radians(self.angle)\n",
    "        magnitude = padding\n",
    "        # this is for the trigonometry function X and Y\n",
    "        dir_vec_x = magnitude * math.cos(theta)\n",
    "        dir_vec_y = magnitude * math.sin(theta)\n",
    "\n",
    "        # adding the direction vector to the center and get an end point for direction\n",
    "        self.direction_end = np.array([self.current_position[0] + dir_vec_x, self.current_position[1] + dir_vec_y],\n",
    "                                      dtype=np.float32)\n",
    "\n",
    "        # this part is only for the render function\n",
    "        self.draw_direction_end = (self.current_position[0] + dir_vec_x, self.current_position[1] + dir_vec_y)\n",
    "        self.center = (int(self.current_position[0]), int(self.current_position[1]))\n",
    "\n",
    "    # updating the direction, line-end according to given angle when called\n",
    "    def get_direction(self):\n",
    "        # as render function demands an int value\n",
    "        center = (int(self.current_position[0]), int(self.current_position[1]))\n",
    "        self.center = center\n",
    "\n",
    "        # the X, Y angular equation\n",
    "        theta = math.radians(self.angle)\n",
    "        magnitude = 30\n",
    "        # here is the X=cos()\n",
    "        directional_vector_x = magnitude * math.cos(theta)\n",
    "        # here is the Y=sin()\n",
    "        directional_vector_y = magnitude * math.sin(theta)\n",
    "\n",
    "        directional_line_end = np.array([center[0] + directional_vector_x, center[1] + directional_vector_y],\n",
    "                                        dtype=np.float32)\n",
    "        self.direction_end = directional_line_end\n",
    "\n",
    "        direction = directional_line_end - center\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        self.direction = direction\n",
    "        self.draw_direction_end = (center[0] + directional_vector_x, center[1] + directional_vector_y)\n",
    "\n",
    "    # for updating the states of the agent when called\n",
    "    def step_update(self, action, speed_factor, range_x, range_y):\n",
    "\n",
    "        # ! if used directional rotational movement\n",
    "        # rotate clockwise\n",
    "        # if action == 0:\n",
    "\n",
    "        #     self.angle += 10\n",
    "        #     self.angle = self.angle % 360\n",
    "        #     # self.get_direction()\n",
    "\n",
    "        # # rotate anti-clockwise\n",
    "        # elif action == 1:\n",
    "        #     self.angle -= 10\n",
    "        #     self.angle = self.angle % 360\n",
    "        #     # self.get_direction()\n",
    "\n",
    "        # # move front\n",
    "        # elif action == 2:\n",
    "\n",
    "        #     self.current_position = self.current_position + self.direction * self.movement_speed * speed_factor\n",
    "        #     # self.get_direction()\n",
    "\n",
    "        # elif action == 3:\n",
    "\n",
    "\n",
    "        # move back\n",
    "        # elif action == 3:\n",
    "        #     self.current_position = self.current_position - self.direction * self.movement_speed\n",
    "            # self.get_direction()\n",
    "\n",
    "        # do nothing / wait\n",
    "        # elif action == 4:\n",
    "        #     pass\n",
    "\n",
    "        movement_speed = self.movement_speed * speed_factor\n",
    "        if action == 0:\n",
    "            self.current_position[0] -= movement_speed\n",
    "        elif action == 1:\n",
    "            self.current_position[0] += movement_speed\n",
    "        elif action == 2:\n",
    "            self.current_position[1] -= movement_speed\n",
    "        elif action == 3:\n",
    "            self.current_position[1] += movement_speed\n",
    "            \n",
    "        self.get_direction()\n",
    "        # self.current_position[0] = np.clip(self.current_position[0], 10, range_x-10)\n",
    "        # self.current_position[1] = np.clip(self.current_position[1], 10, range_y-10)\n",
    "\n",
    "    # this function returns all the state needed for the observations\n",
    "    # ! can be changed with need for the algorithm\n",
    "    def get_agent_state(self):\n",
    "\n",
    "        agent_state = {\n",
    "            'agent_id': self.index,\n",
    "            'agent_name': self.agent,\n",
    "            'agent_move_speed': self.movement_speed,\n",
    "            'agent_current_position': self.current_position,\n",
    "            'agent_angle': self.angle\n",
    "        }\n",
    "\n",
    "        return agent_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 300}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = SCREEN_WIDTH\n",
    "        self.screen_height = SCREEN_HEIGHT\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # self.observation_space = Dict({\n",
    "        #     \"predator_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "        #                              high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "        #                              dtype=np.float32),\n",
    "\n",
    "        #     \"bullet_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "        #                            high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "        #                            dtype=np.float32),\n",
    "\n",
    "        #     \"target_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "        #                            high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "        #                            dtype=np.float32),\n",
    "        # })\n",
    "\n",
    "        self.observation_space = Box(low=np.zeros(42, np.float32), \n",
    "                                    high=np.array([1000 for _ in range(42)], dtype=np.float32), dtype=np.float32)\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_agent = Agent('predator', 0)\n",
    "        self.predator_total_reward = 0\n",
    "        self.cast_ray = CastRay()\n",
    "\n",
    "        self.obs = None\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.animation_time = None\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # for the wall initializations\n",
    "        self.wall = Walls(pygame)\n",
    "        self.walls = None\n",
    "\n",
    "        self.turret = Turret(SCREEN_WIDTH, SCREEN_HEIGHT)\n",
    "        self.bullet = self.turret.get_bullets()\n",
    "\n",
    "    def _get_obs(self):\n",
    "\n",
    "        if len(self.bullet) == 1:\n",
    "            bullet_pos = self.bullet[0].pos\n",
    "        else:\n",
    "            bullet_pos = [0, 0]\n",
    "\n",
    "        # observation = {\n",
    "        #     \"predator_position\": self.predator_agent.current_position.tolist(),\n",
    "        #     \"bullet_position\": bullet_pos,  # get bullet position\n",
    "        #     \"target_position\": self.turret.position.tolist(),  # get the main target position\n",
    "        # }\n",
    "        # object_list = self.walls\n",
    "        # object_list.extend()\n",
    "        lengths, _ = self.cast_ray.cast_rays(self.predator_agent, self.walls)\n",
    "\n",
    "        observation = []\n",
    "        observation.extend(self.predator_agent.current_position.tolist())\n",
    "        observation.extend(bullet_pos)\n",
    "        observation.extend(self.turret.position.tolist())\n",
    "        observation.extend(lengths)\n",
    "        # print(f'observation:{observation}')\n",
    "        return observation\n",
    "\n",
    "    # def _get_info(self):\n",
    "    #     distance = 10000\n",
    "    #     self.goal_seen = is_ray_blocked(self.predator_agent.current_position, self.goal_coordinate, self.walls)\n",
    "    #     if self.goal_seen:\n",
    "    #         direction = self.goal_coordinate - self.predator_agent.current_position\n",
    "    #         distance = np.linalg.norm(direction)\n",
    "    #\n",
    "    #     info = {\n",
    "    #         \"goal_seen\": self.goal_seen,\n",
    "    #         \"distance\": distance,\n",
    "    #         \"vision_blocked\": not self.goal_seen,\n",
    "    #     }\n",
    "    #     # print(f'info: {info}')\n",
    "    #     return info\n",
    "\n",
    "    def get_reward(self, reward, done):\n",
    "        bullet_pos = 0\n",
    "        if len(self.bullet) == 1:\n",
    "            bullet_pos = self.bullet[0].pos\n",
    "        else:\n",
    "            bullet_pos = self.turret.position\n",
    "\n",
    "        if np.linalg.norm(np.abs(self.predator_agent.current_position - bullet_pos)) < self.predator_agent.radius + self.bullet[0].radius:\n",
    "            self.turret.destroy_bullet(self.bullet[0])\n",
    "            reward -= 50\n",
    "            done = True\n",
    "\n",
    "        if np.linalg.norm(np.abs(self.predator_agent.current_position - self.turret.position)) < self.predator_agent.radius + self.turret.radius + 20:\n",
    "            reward += 200\n",
    "            done = True\n",
    "\n",
    "        reward += 0.01\n",
    "\n",
    "        distance_between_targets = np.linalg.norm(np.abs(self.predator_agent.current_position - self.turret.position))\n",
    "        # print(10/distance_between_targets)\n",
    "        reward += (10/distance_between_targets)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def reset(self, seed=None, option=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.wall.clear_walls()\n",
    "        self.walls = self.wall.make_wall(LEVEL_5_WALLS)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "        self.animation_time = time.time()\n",
    "\n",
    "        # for predator in self.predator_agents:\n",
    "        self.predator_agent.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "        # self.predator_agent.movement_speed = 300\n",
    "        self.turret.rotate_turret(self.predator_agent.center)\n",
    "\n",
    "        # all the variable values inside the observation space needs to be sent inside the observation variable\n",
    "        # for this level purpose we decided to add the dictionary observation\n",
    "        # set the observation to a dictionary\n",
    "        observation = self._get_obs()\n",
    "        # info = self._get_info()\n",
    "\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # initializing the return variables\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "\n",
    "        current_animation_time = time.time()\n",
    "        difference_in_animaton_time = current_animation_time - self.animation_time\n",
    "        self.animation_time = current_animation_time\n",
    "        # print(difference_in_animaton_time)\n",
    "        \n",
    "        elapsed_time = current_time - self.start_time\n",
    "\n",
    "        self.predator_agent.step_update(action, speed_factor=difference_in_animaton_time, range_x=self.screen_width, range_y=self.screen_height)\n",
    "        self.predator_agent = detect_collision(self.predator_agent, self.walls)\n",
    "\n",
    "        if len(self.turret.get_bullets()) == 0:\n",
    "            self.turret.shoot()\n",
    "\n",
    "        self.bullet[0].move(difference_in_animaton_time)\n",
    "        # if np.linalg.norm(np.abs(self.predator_agent.center - self.bullet[0].center)) < self.predator_agent.radius + self.bullet[0].radius:\n",
    "\n",
    "        # observation needs to be set a dictionary\n",
    "\n",
    "        self.total_steps += 1\n",
    "        reward, done = self.get_reward(reward, done)\n",
    "\n",
    "        if elapsed_time >= self.total_running_time:\n",
    "            reward -= 20\n",
    "            done = True\n",
    "\n",
    "        # getting observation and info\n",
    "        observation = self._get_obs()\n",
    "        # info = self._get_info()\n",
    "\n",
    "        self.predator_total_reward = reward\n",
    "        self.obs = observation\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'rgb_array':\n",
    "            self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.font.init()\n",
    "\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        screen.fill(WHITE)\n",
    "\n",
    "        # for the predator\n",
    "        predator = self.predator_agent\n",
    "        predator_rect = pygame.draw.circle(screen, RED, predator.center, predator.radius)\n",
    "        # pygame.draw.line(screen, RED, predator.center, predator.draw_direction_end, 5)\n",
    "\n",
    "        # for turret\n",
    "        pygame.draw.circle(screen, (0, 255, 0), self.turret.center, self.turret.radius)\n",
    "        pygame.draw.line(screen, (0, 255, 0), self.turret.center, self.turret.rotate_turret(predator.center), 4)\n",
    "\n",
    "        \n",
    "        # for the bullet\n",
    "        # bullet_rect_alt = pygame.draw.circle(screen, WHITE, (0, 0), 10)\n",
    "        if len(self.bullet) != 0:\n",
    "            bullet_rect = pygame.draw.circle(screen, (255, 255, 0), self.bullet[0].center, self.bullet[0].radius)\n",
    "            self.turret.auto_destroy()\n",
    "            for wall in self.walls:\n",
    "                if bullet_rect.colliderect(wall):\n",
    "                    self.turret.destroy_bullet(self.bullet[0])\n",
    "            # if bullet_rect.collidelist(self.walls) or bullet_rect.colliderect(predator_rect):\n",
    "            #     self.turret.destroy_bullet(self.bullet[0])\n",
    "        \n",
    "        lengths, angles = self.cast_ray.cast_rays(predator, self.walls)\n",
    "\n",
    "        for a, l in zip(angles, lengths):\n",
    "            end_point = (int(predator.center[0] + l * math.cos(math.radians(a))),\n",
    "                        int(predator.center[1] + l * math.sin(math.radians(a))))\n",
    "\n",
    "            pygame.draw.line(screen, RED, predator.center, end_point)\n",
    "\n",
    "        for key, wall in LEVEL_5_WALLS.items():\n",
    "            pygame.draw.rect(screen, BLUE, (wall['x'], wall['y'], wall['width'], wall['height']))\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "\n",
    "            font = pygame.font.Font(None, 18)\n",
    "\n",
    "            text_surface = font.render(f\"Reward: {self.predator_total_reward: .5f} \", True, (0, 0, 0))\n",
    "\n",
    "            text_rect = text_surface.get_rect()\n",
    "\n",
    "            text_rect.center = (self.screen_width - 200, 10)\n",
    "\n",
    "            screen.blit(text_surface, text_rect)\n",
    "            self.window.blit(screen, screen.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # this part is to fix the fps of rendering\n",
    "            # self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "        else:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.font.quit()\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_angle = 0  # 65 degrees to the left\n",
    "end_angle = 350  # 65 degrees to the right\n",
    "angle_step = 10  # One ray every 10 degrees\n",
    "ray_angles = np.arange(start_angle, end_angle + angle_step, angle_step).tolist()\n",
    "ray_angles = [angle%360 for angle in ray_angles]\n",
    "\n",
    "len(ray_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([800 for _ in range(42)], dtype=np.float32)\n",
    "len(arr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173\n",
      "14.885931757955543\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv('human')\n",
    "env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    _, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "print(env.total_steps)\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs', 'Final_3')\n",
    "model_path = os.path.join('Training', 'Models', 'Final_3')\n",
    "best_save_path = os.path.join('Training', 'Models', 'Final_3', 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=100000,\n",
    "  save_path=model_path,\n",
    "  name_prefix=\"rl_model\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = GameEnv('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env2, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\Final_3\\DQN_3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\CSE_465\\directionall_movement.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env2\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m50000000\u001b[39m, callback\u001b[39m=\u001b[39mcheckpoint_callback)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect_rollouts(\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv,\n\u001b[0;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_freq,\n\u001b[0;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_noise,\n\u001b[0;32m    316\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_starts,\n\u001b[0;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer,\n\u001b[0;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    541\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    543\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    547\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\CSE_465\\directionall_movement.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs \u001b[39m=\u001b[39m observation\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_frame()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m \u001b[39mreturn\u001b[39;00m observation, reward, done, truncated, info\n",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\CSE_465\\directionall_movement.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m     pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mrect(screen, BLUE, (wall[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m], wall[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m], wall[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m], wall[\u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=234'>235</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=236'>237</a>\u001b[0m     font \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mfont\u001b[39m.\u001b[39mFont(\u001b[39mNone\u001b[39;00m, \u001b[39m18\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=238'>239</a>\u001b[0m     text_surface \u001b[39m=\u001b[39m font\u001b[39m.\u001b[39mrender(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReward: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredator_total_reward\u001b[39m:\u001b[39;00m\u001b[39m .5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/directionall_movement.ipynb#X16sZmlsZQ%3D%3D?line=240'>241</a>\u001b[0m     text_rect \u001b[39m=\u001b[39m text_surface\u001b[39m.\u001b[39mget_rect()\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pygame\\pkgdata.py:65\u001b[0m, in \u001b[0;36mgetResource\u001b[1;34m(identifier, pkgname)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m# When pyinstaller (or similar tools) are used, resource_exists may raise\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m# NotImplemented error\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mif\u001b[39;00m resource_exists(pkgname, identifier):\n\u001b[0;32m     66\u001b[0m         \u001b[39mreturn\u001b[39;00m resource_stream(pkgname, identifier)\n\u001b[0;32m     67\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pkg_resources\\__init__.py:1205\u001b[0m, in \u001b[0;36mResourceManager.resource_exists\u001b[1;34m(self, package_or_requirement, resource_name)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresource_exists\u001b[39m(\u001b[39mself\u001b[39m, package_or_requirement, resource_name):\n\u001b[0;32m   1204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Does the named resource exist?\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1205\u001b[0m     \u001b[39mreturn\u001b[39;00m get_provider(package_or_requirement)\u001b[39m.\u001b[39mhas_resource(resource_name)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pkg_resources\\__init__.py:1503\u001b[0m, in \u001b[0;36mNullProvider.has_resource\u001b[1;34m(self, resource_name)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_resource\u001b[39m(\u001b[39mself\u001b[39m, resource_name):\n\u001b[1;32m-> 1503\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_path, resource_name))\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\pkg_resources\\__init__.py:1715\u001b[0m, in \u001b[0;36mDefaultProvider._has\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_has\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m-> 1715\u001b[0m     \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path)\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env2.reset()\n",
    "model.learn(total_timesteps=50000000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
