{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_name, agent_index):\n",
    "        self.index = agent_index\n",
    "        self.agent = agent_name\n",
    "        self.health = None\n",
    "        self.isHit = False\n",
    "        self.move = True\n",
    "        self.movement_speed = 1.00\n",
    "        self.previous_position = np.array([0, 0], dtype=np.float32)\n",
    "        self.current_position = None\n",
    "        self.same_position = False\n",
    "        self.current_step = 0\n",
    "        self.action = None\n",
    "        pass\n",
    "\n",
    "    def agent_action(self, action):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def agent_update(self, step, action, width, height):\n",
    "        if step > 0:\n",
    "            if (self.previous_position != self.current_position).all():\n",
    "                self.previous_position = self.current_position\n",
    "                self.same_position = False\n",
    "\n",
    "                if action:\n",
    "                    self.step_update(action)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                self.same_position = True\n",
    "\n",
    "    def agent_reset(self, width, height):\n",
    "        padding = 30\n",
    "        self.current_position = np.array(\n",
    "            [np.random.uniform(30, width - padding), np.random.uniform(30, width - padding)], dtype=np.float32)\n",
    "\n",
    "    def step_update(self, action, range_x, range_y):\n",
    "\n",
    "        if action == 0:\n",
    "            self.current_position[0] -= self.movement_speed\n",
    "        elif action == 1:\n",
    "            self.current_position[0] += self.movement_speed\n",
    "        elif action == 2:\n",
    "            self.current_position[1] -= self.movement_speed\n",
    "        elif action == 3:\n",
    "            self.current_position[1] += self.movement_speed\n",
    "        \n",
    "        self.current_position[0] = np.clip(self.current_position[0], 0, range_x)\n",
    "        self.current_position[1] = np.clip(self.current_position[1], 0, range_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gymnasium.spaces import Discrete, Box, MultiDiscrete\n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, screen_width=400, screen_height=400, render_mode='human'):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # defining the observation and action spaces for all the agents\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(low=np.array([0, 0, 0, 0], dtype=np.float32),\n",
    "                                    high=np.array([self.screen_width, self.screen_height, self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "        # the pygame window should be initialized in the render function\n",
    "\n",
    "        # setting the total number of agent\n",
    "        \n",
    "        self.number_of_prey = 1\n",
    "        self.number_of_predator = 1\n",
    "        self.prey_agents = []\n",
    "        self.predator_agents = []\n",
    "        self.number_of_agents = self.number_of_prey + self.number_of_prey\n",
    "\n",
    "        # if self.number_of_prey > 0 and self.number_of_predator > 0:\n",
    "        #     self.agent_init()\n",
    "        # else:\n",
    "        #     self.prey_agents.append(Agent('prey', 0))\n",
    "        #     self.predator_agents.append(Agent('predator', 0))\n",
    "\n",
    "        # setting the total number of obstacles\n",
    "        self.total_obstacles = None\n",
    "\n",
    "        # keeping a counter to save the total steps\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # initializing the pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # setting the screen size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Multi Agent Environment(simple)')\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        # initializing the font\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "\n",
    "    def agent_init(self):\n",
    "        # initializing all the agents\n",
    "        prey_agents = []\n",
    "        predator_agents = []\n",
    "\n",
    "        for i in range(0, self.number_of_prey):\n",
    "            agent = Agent('prey', i)\n",
    "            prey_agents.append(agent)\n",
    "\n",
    "        for i in range(0, self.number_of_predator):\n",
    "            agent = Agent('predator', i)\n",
    "            predator_agents.append(agent)\n",
    "\n",
    "        self.prey_agents = prey_agents\n",
    "        self.predator_agents = predator_agents\n",
    "\n",
    "    def set_agent_number(self, prey_number, predator_number):\n",
    "        self.number_of_predator = predator_number\n",
    "        self.number_of_prey = prey_number\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        self.total_steps = 0\n",
    "        observation = []\n",
    "\n",
    "        for prey in self.prey_agents:\n",
    "            prey.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "            observation.append([prey.index, prey.agent, prey.current_position])\n",
    "\n",
    "        for predator in self.predator_agents:\n",
    "            predator.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "            observation.append([predator.index, predator.agent, predator.current_position])\n",
    "\n",
    "        return observation, seed\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        observation = []\n",
    "\n",
    "        prey_actions, predator_actions = action\n",
    "\n",
    "        for prey, action in zip(self.prey_agents, prey_actions):\n",
    "\n",
    "            # print(f'prey_{prey.index} = action:{action} current_position: {prey.current_position}')\n",
    "            prey.step_update(action=action, range_x=self.screen_width - 10, range_y=self.screen_height - 10)\n",
    "            # print(f'prey_{prey.index}: new_position: {prey.current_position}')\n",
    "\n",
    "            observation.append({'index': prey.index, 'name': prey.agent, 'position': prey.current_position})\n",
    "                \n",
    "        for predator, action in zip(self.predator_agents, predator_actions):\n",
    "\n",
    "            # print(f'predator_{predator.index} = action:{action} current_position: {predator.current_position}')\n",
    "            predator.step_update(action=action, range_x=self.screen_width - 10, range_y=self.screen_height - 10)\n",
    "            # print(f'predator_{predator.index}: new_position: {predator.current_position}')\n",
    "            \n",
    "            observation.append({'index': predator.index, 'name': predator.agent, 'position': predator.current_position})\n",
    "        \n",
    "        self.total_steps += 1\n",
    "\n",
    "        done = False\n",
    "        reward = 0.00\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        # print(self.total_steps)\n",
    "        self.render()\n",
    "\n",
    "        return observation, reward, done, truncated, info\n",
    "        \n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            screen = self.screen\n",
    "\n",
    "            # clear screen\n",
    "            screen.fill((255, 255, 255))\n",
    "\n",
    "            for prey in self.prey_agents:\n",
    "                pos_x, pos_y = prey.current_position\n",
    "                prey_radius = 10\n",
    "                pygame.draw.circle(screen, (0, 0, 255), (int(pos_x), int(pos_y)), prey_radius)\n",
    "\n",
    "            for predator in self.predator_agents:\n",
    "                pos_x, pos_y = predator.current_position\n",
    "                predator_radius = 10\n",
    "\n",
    "                pygame.draw.circle(screen, (255, 0, 0), (int(pos_x), int(pos_y)), predator_radius)\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "done = False\n",
    "number_of_prey = 2\n",
    "number_of_predator = 3\n",
    "\n",
    "env.set_agent_number(prey_number=number_of_prey, predator_number=number_of_predator)\n",
    "env.agent_init()\n",
    "env.reset()\n",
    "\n",
    "while not done:\n",
    "    prey_action = []\n",
    "    predator_action = []\n",
    "    for i in range(0, number_of_prey):\n",
    "        prey_action.append(env.action_space.sample())\n",
    "    \n",
    "    for i in range(0, number_of_predator):\n",
    "        predator_action.append(env.action_space.sample())\n",
    "\n",
    "    action = [prey_action, predator_action]\n",
    "\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    print(obs)\n",
    "    # env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 'prey', array([249.11028, 316.7625 ], dtype=float32)],\n",
       "  [1, 'prey', array([286.55774,  70.27204], dtype=float32)],\n",
       "  [0, 'predator', array([152.40836 , 122.442825], dtype=float32)],\n",
       "  [1, 'predator', array([184.71822, 320.53717], dtype=float32)],\n",
       "  [2, 'predator', array([185.87051, 259.77145], dtype=float32)]],\n",
       " 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "number_of_prey = 2\n",
    "number_of_predator = 3\n",
    "\n",
    "env.set_agent_number(prey_number=number_of_prey, predator_number=number_of_predator)\n",
    "env.agent_init()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = os.path.join('Training', 'Models', 'test_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\Test\\test.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/Test/test.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:246\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    236\u001b[0m     \u001b[39mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    237\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    242\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    243\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    244\u001b[0m     iteration \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 246\u001b[0m     total_timesteps, callback \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_learn(\n\u001b[0;32m    247\u001b[0m         total_timesteps,\n\u001b[0;32m    248\u001b[0m         callback,\n\u001b[0;32m    249\u001b[0m         reset_num_timesteps,\n\u001b[0;32m    250\u001b[0m         tb_log_name,\n\u001b[0;32m    251\u001b[0m         progress_bar,\n\u001b[0;32m    252\u001b[0m     )\n\u001b[0;32m    254\u001b[0m     callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    256\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:424\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m# pytype: disable=annotation-type-mismatch\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset()  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[39m# pytype: enable=annotation-type-mismatch\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seeds[env_idx])\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     78\u001b[0m \u001b[39m# Seeds are only used once\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_seeds()\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:106\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[1;34m(self, env_idx, obs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeys:\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs\n\u001b[0;32m    107\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_obs[key][env_idx] \u001b[39m=\u001b[39m obs[key]\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
