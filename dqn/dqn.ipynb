{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygame_sdl2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\dqn\\dqn.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpygame_sdl2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m pygame_sdl2\u001b[39m.\u001b[39minit()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pygame_sdl2'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import random\n",
    "import pygame\n",
    "import pygame_sdl2\n",
    "pygame_sdl2.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredatorPreyENV(gym.Env):\n",
    "    # The primary purpose of PredatorPreyENV is to store configuration settings and sensitive information\n",
    "    #  Python class named PredatorPreyENV that inherits from the custom gym.Env\n",
    "    def __init__(self, screen_width=800, screen_height=600, circle_radius=150):\n",
    "        super(PredatorPreyENV, self).__init__()\n",
    "\n",
    "        # Define the screen dimensions\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.circle_radius = circle_radius\n",
    "\n",
    "\n",
    "        # defining the agent policies\n",
    "        self.blue_dot_radius = 40\n",
    "        self.direction_line_length = 40\n",
    "        self.blue_dot_health = 50\n",
    "        self.red_dot_health = 50\n",
    "        \n",
    "        # Define the attack damage of the red dot\n",
    "        self.red_dot_attack_dmg = 10  # Adjust this value as needed\n",
    "        \n",
    "        # Define movement speeds for blue and red dots\n",
    "        self.blue_dot_move_speed = 0.8\n",
    "        self.red_dot_move_speed = 0.1\n",
    "\n",
    "        # Define 4 discreet action space (left, right, up, down)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Define observation space (positions of blue dot and red dot)\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0], dtype=np.float32), high=np.array(\n",
    "            [self.screen_width / 2, self.screen_height, self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        # Initialize the pygame window\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Dots Moving Environment')\n",
    "\n",
    "        # Initialize the positions of the blue and red dots\n",
    "        # self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        # self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        # Define grid line properties\n",
    "        self.grid_color = (210, 210, 210)\n",
    "        self.grid_spacing = 40  # Adjust this value to change the grid spacing\n",
    "\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "\n",
    "        self.total_reward = 0\n",
    "    #     keep track of the cumulative reward earned by the agent as it interacts with the environment\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        super().reset(seed=seed)\n",
    "        # Reset the positions of the blue and red dots at start of each episodes\n",
    "        # self.blue_dot_pos = np.array([0, 0], dtype=np.float32)\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 2, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        # self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        # Randomly set the initial position of the red dot along the circumference of the circle\n",
    "        angle = np.random.uniform(0, 2 * np.pi)\n",
    "        x = self.blue_dot_pos[0] + self.circle_radius * np.cos(angle)\n",
    "        y = self.blue_dot_pos[1] + self.circle_radius * np.sin(angle)\n",
    "        self.red_dot_pos = np.array([x, y], dtype=np.float32)\n",
    "\n",
    "        # Reset the position of the red dot to the middle of the screen\n",
    "        # self.red_dot_pos = np.array([self.screen_width / 2, self.screen_height / 2], dtype=np.float32)\n",
    "        self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        self.blue_dot_health = 50\n",
    "        self.total_reward = 0\n",
    "        return np.concatenate([self.blue_dot_pos, self.red_dot_pos]), seed \n",
    "\n",
    "    # This function essentially describes how the blue and red dots interact\n",
    "    # based on the selected actions, handle collisions, and update their positions and rewards within the environment.\n",
    "    def step(self, action_blue_dot):  #per second 1 frame pass what happens determines step function\n",
    "        # truncated == false\n",
    "        # Define the movement speed\n",
    "        # self used to access variables\n",
    "        move_speed_blue = 0.8   #blue\n",
    "        move_speed_red = 0.1   # red\n",
    "\n",
    "        action_red_dot = 0\n",
    "\n",
    "        # Separate the action for blue and red dots\n",
    "        # action_blue_dot, action_red_dot = action\n",
    "        # POLICIES\n",
    "        if action_blue_dot == 0:  # Move blue dot left\n",
    "            self.blue_dot_pos[0] -= move_speed_blue\n",
    "        elif action_blue_dot == 1:  # Move blue dot right\n",
    "            self.blue_dot_pos[0] += move_speed_blue\n",
    "        elif action_blue_dot == 2:  # Move blue dot up\n",
    "            self.blue_dot_pos[1] -= move_speed_blue\n",
    "        elif action_blue_dot == 3:  # Move blue dot down\n",
    "            self.blue_dot_pos[1] += move_speed_blue\n",
    "\n",
    "\n",
    "        # Calculate the direction vector from the red dot to the blue dot by subtracting\n",
    "        direction = self.blue_dot_pos - self.red_dot_pos\n",
    "\n",
    "        # Normalize the direction vector by dividing the vector by its magnitude (length) to turn it into a unit vector\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        # normalized vector (direction) indicates the direction from the red dot to the blue dot.\n",
    "        #print(\"Direction: \", direction)\n",
    "        distance_between_centers = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "        # calculates the Euclidean distance between the centers of the blue and red dots,\n",
    "        # measures how far apart the two dots are in terms of pixel distance.\n",
    "\n",
    "        # Calculate the distance between the centers of the blue and red dots\n",
    "        distance_between_centers = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "\n",
    "        # Check if the red dot crosses the circle boundary\n",
    "        if distance_between_centers > self.circle_radius:\n",
    "            # Penalize the agent and set the 'done' flag if the red dot crosses the circle\n",
    "            done = True\n",
    "            reward -= 10\n",
    "\n",
    "        # radii for collision detection. The red_dot_radius is set to be 15 pixels smaller than the blue_dot_radius.\n",
    "        blue_dot_radius = self.blue_dot_radius\n",
    "        red_dot_radius = blue_dot_radius - 15\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Check if the dots collide with each other\n",
    "        # checking if the distance between the centers of the blue and red dots (distance_between_centers)\n",
    "        # is less than the sum of their radii (blue_dot_radius + red_dot_radius)\n",
    "        if distance_between_centers < blue_dot_radius + red_dot_radius:\n",
    "            # Separate the dots by moving the red dot away from the blue dot\n",
    "            self.red_dot_pos -= -50.0 + move_speed_red * direction\n",
    "            self.blue_dot_health -= self.red_dot_attack_dmg\n",
    "            if(self.blue_dot_health <= 0):\n",
    "                done = True\n",
    "            reward -= 5\n",
    "            #print(\"collision\")\n",
    "\n",
    "        else:\n",
    "            # Move the red dot towards the blue dot with a fixed speed\n",
    "            self.red_dot_pos += move_speed_red * direction\n",
    "\n",
    "        # Clip blue dot position to stay within the first half of the screen\n",
    "        self.blue_dot_pos[0] = np.clip(self.blue_dot_pos[0], 0, self.screen_width)\n",
    "        self.blue_dot_pos[1] = np.clip(self.blue_dot_pos[1], 0, self.screen_height)\n",
    "\n",
    "        # Clip red dot position to stay within the entire screen\n",
    "        self.red_dot_pos = np.clip(self.red_dot_pos, [0, 0], [self.screen_width, self.screen_height])\n",
    "\n",
    "        # Define a simple reward function (e.g., distance between the two dots)\n",
    "        # reward = -np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Check if the dots are close to each other (you can adjust the distance threshold as needed)\n",
    "        # done = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos) < 10\n",
    "        return np.concatenate([self.blue_dot_pos, self.red_dot_pos]), reward, done, False, {}\n",
    "    # done flag indicating the end of the episode, and an empty dictionary ({}) for additional information\n",
    "\n",
    "    def display_total_reward(self):\n",
    "        text_surface = self.font.render(f\"Reward: {self.total_reward: .2f} Blue Health: {self.blue_dot_health}\", True, (0, 0, 0))\n",
    "        # the reward and blue dot health values text\n",
    "        text_rect = text_surface.get_rect()\n",
    "        # position the text on the pygame window.\n",
    "        text_rect.center = (self.screen_width - 200, 10)\n",
    "        self.screen.blit(text_surface, text_rect)\n",
    "    # The blit method is used to draw the text surface (text_surface) onto the pygame window (self.screen)\n",
    "\n",
    "    # render function is responsible for creating a visual representation of the environment\n",
    "    def render(self, action_blue, action_red):\n",
    "        # Clear the screen\n",
    "        self.screen.fill((229, 222, 248))\n",
    "\n",
    "        # Draw grid lines\n",
    "        for x in range(0, self.screen_width, self.grid_spacing):\n",
    "            pygame.draw.line(self.screen, self.grid_color, (x, 0), (x, self.screen_height), 1)\n",
    "        for y in range(0, self.screen_height, self.grid_spacing):\n",
    "            pygame.draw.line(self.screen, self.grid_color, (0, y), (self.screen_width, y), 1)\n",
    "\n",
    "        # Draw blue dot\n",
    "        pygame.draw.circle(self.screen, (141,144,226), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])), self.blue_dot_radius)\n",
    "\n",
    "        # Draw red dot\n",
    "        pygame.draw.circle(self.screen, (158, 50, 90), (int(self.red_dot_pos[0]), int(self.red_dot_pos[1])), self.blue_dot_radius - 10)\n",
    "\n",
    "        # calculating the position of facing direction lines\n",
    "        blue_dot_direction_end = tuple(map(int, self.blue_dot_pos + self.direction_line_length * action_blue))\n",
    "        red_dot_direction_end = tuple(map(int, self.red_dot_pos + self.direction_line_length * action_red))\n",
    "\n",
    "        # direction line draw\n",
    "        pygame.draw.line(self.screen, (0, 0, 255), tuple(map(int, self.blue_dot_pos)), blue_dot_direction_end, 2)\n",
    "        pygame.draw.line(self.screen, (255, 0, 0), tuple(map(int, self.red_dot_pos)), red_dot_direction_end, 2)\n",
    "\n",
    "        self.display_total_reward()\n",
    "\n",
    "        # Update the display\n",
    "        pygame.display.update()\n",
    "\n",
    "    # def close(self):\n",
    "    #     pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PredatorPreyENV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Models', 'DQN_Model')\n",
    "log_path = os.path.join('Training', 'DQN_Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env=env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.82e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.558    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 4143     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 23270    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.05e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.0799   |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 4655     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 48425    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 6.09e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 1850     |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 73043    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 18260    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.93e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 1419     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 94952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 23737    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.98e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 1189     |\n",
      "|    time_elapsed     | 100      |\n",
      "|    total_timesteps  | 119512   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00643  |\n",
      "|    n_updates        | 29877    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.97e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 1118     |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 143353   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00882  |\n",
      "|    n_updates        | 35838    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.81e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 1089     |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total_timesteps  | 162635   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00259  |\n",
      "|    n_updates        | 40658    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.62e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 1068     |\n",
      "|    time_elapsed     | 168      |\n",
      "|    total_timesteps  | 179868   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00326  |\n",
      "|    n_updates        | 44966    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.63e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 1042     |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total_timesteps  | 202833   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 50708    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.61e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 1020     |\n",
      "|    time_elapsed     | 219      |\n",
      "|    total_timesteps  | 224225   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00199  |\n",
      "|    n_updates        | 56056    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.55e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 1001     |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total_timesteps  | 244396   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00416  |\n",
      "|    n_updates        | 61098    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.48e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 987      |\n",
      "|    time_elapsed     | 266      |\n",
      "|    total_timesteps  | 263085   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00506  |\n",
      "|    n_updates        | 65771    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.44e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 972      |\n",
      "|    time_elapsed     | 290      |\n",
      "|    total_timesteps  | 282674   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00333  |\n",
      "|    n_updates        | 70668    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.37e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 962      |\n",
      "|    time_elapsed     | 312      |\n",
      "|    total_timesteps  | 300568   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00383  |\n",
      "|    n_updates        | 75141    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.3e+03  |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 955      |\n",
      "|    time_elapsed     | 333      |\n",
      "|    total_timesteps  | 318300   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 79574    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.22e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 949      |\n",
      "|    time_elapsed     | 352      |\n",
      "|    total_timesteps  | 334315   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00432  |\n",
      "|    n_updates        | 83578    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.2e+03  |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 943      |\n",
      "|    time_elapsed     | 374      |\n",
      "|    total_timesteps  | 353672   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00758  |\n",
      "|    n_updates        | 88417    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.19e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 936      |\n",
      "|    time_elapsed     | 398      |\n",
      "|    total_timesteps  | 373441   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00274  |\n",
      "|    n_updates        | 93360    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.12e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 932      |\n",
      "|    time_elapsed     | 416      |\n",
      "|    total_timesteps  | 388870   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00241  |\n",
      "|    n_updates        | 97217    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.1e+03  |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 928      |\n",
      "|    time_elapsed     | 439      |\n",
      "|    total_timesteps  | 407603   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00274  |\n",
      "|    n_updates        | 101900   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.03e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 924      |\n",
      "|    time_elapsed     | 457      |\n",
      "|    total_timesteps  | 422850   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00493  |\n",
      "|    n_updates        | 105712   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.06e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 920      |\n",
      "|    time_elapsed     | 483      |\n",
      "|    total_timesteps  | 445207   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0038   |\n",
      "|    n_updates        | 111301   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.03e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 504      |\n",
      "|    total_timesteps  | 462954   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00657  |\n",
      "|    n_updates        | 115738   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.99e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 914      |\n",
      "|    time_elapsed     | 523      |\n",
      "|    total_timesteps  | 478755   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 119688   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.94e+03 |\n",
      "|    ep_rew_mean      | -25      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 911      |\n",
      "|    time_elapsed     | 542      |\n",
      "|    total_timesteps  | 494356   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00162  |\n",
      "|    n_updates        | 123588   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1ec13cb0110>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} Std reward: {std_reward}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
