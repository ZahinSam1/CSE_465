{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DotEnv(gym.Env):\n",
    "    def __init__(self, screen_width=700, screen_height=700, render_mode='human'):\n",
    "        super(DotEnv, self).__init__()\n",
    "\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.direction_line_length = 20\n",
    "        self.game_speed = 1\n",
    "        self.test_health = 50\n",
    "\n",
    "        self.blue_dot_radius = 30\n",
    "        self.blue_dot_health = 50\n",
    "        self.blue_dot_attack_dmg = 1\n",
    "        self.blue_dot_search_radius = 100\n",
    "        self.blue_dot_turn_rate = None\n",
    "        self.blue_dot_stamina = 50\n",
    "        self.blue_dot_stamina_recovery_rate = 15\n",
    "        self.blue_dot_pos_prev = None\n",
    "\n",
    "        self.food_radius = 20\n",
    "        self.food_positions = []  # List to store food positions\n",
    "        self.food_types = []  # List to store food types ('pink', 'green', 'yellow')\n",
    "        self.food_effects = []  # List to store food effects (1: increase health, -1: decrease health, 2: speed boost)\n",
    "\n",
    "        self.eaten_food = 0  # Variable to keep track of eaten food\n",
    "\n",
    "        self.food_reward = 100  # Reward for collecting pink (edible) food\n",
    "        self.green_food_penalty = -50  # Penalty for collecting green (inedible) food\n",
    "\n",
    "        self.food_regeneration_interval = 10  # Regenerate food every 10 seconds\n",
    "        self.last_food_regeneration_time = 0\n",
    "\n",
    "        self.max_food_count = 10\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0], dtype=np.float32),\n",
    "                                            high=np.array([self.screen_width / 2, self.screen_height],\n",
    "                                                          dtype=np.float32),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.Surface((self.screen_width, self.screen_height))  # Create a surface for rendering\n",
    "        pygame.display.set_caption('Dots Moving Environment')\n",
    "\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.speed_boost_timer = 0  # Initialize the speed boost timer\n",
    "\n",
    "        for _ in range(self.max_food_count):\n",
    "            self.spawn_food()\n",
    "\n",
    "    def spawn_food(self):\n",
    "        while True:\n",
    "            food_position = np.array(\n",
    "                [random.uniform(0, self.screen_width), random.uniform(0, self.screen_height)],\n",
    "                dtype=np.float32)\n",
    "            if np.linalg.norm(\n",
    "                    food_position - np.array([self.screen_width / 2, self.screen_height / 2])) < int(\n",
    "                self.screen_width / 2) - self.food_radius:\n",
    "                food_type = random.choice(['pink', 'green', 'yellow'])\n",
    "                self.food_positions.append(food_position)\n",
    "                self.food_types.append(food_type)\n",
    "\n",
    "                if food_type == 'pink':\n",
    "                    food_effect = 1\n",
    "                elif food_type == 'green':\n",
    "                    food_effect = -1\n",
    "                else:\n",
    "                    food_effect = 2\n",
    "\n",
    "                self.food_effects.append(food_effect)\n",
    "                break\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        super().reset(seed=seed)\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        self.blue_dot_health = 50\n",
    "\n",
    "        self.food_positions = []\n",
    "        self.food_types = []\n",
    "        self.food_effects = []\n",
    "\n",
    "        for _ in range(self.max_food_count):\n",
    "            self.spawn_food()\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.speed_boost_timer = 0  # Reset the speed boost timer\n",
    "\n",
    "        self.last_food_regeneration_time = time.time()  # Reset the food regeneration timer\n",
    "        self.eaten_food = 0  # Reset eaten food count\n",
    "\n",
    "        observation = np.concatenate([self.blue_dot_pos])\n",
    "\n",
    "        return [observation, seed]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        action_blue_dot = action\n",
    "\n",
    "        move_speed = 0.9 * self.game_speed\n",
    "\n",
    "        prev_blue_dot_pos = np.copy(self.blue_dot_pos)\n",
    "\n",
    "        if action_blue_dot == 0:\n",
    "            self.blue_dot_pos[0] -= move_speed\n",
    "        elif action_blue_dot == 1:\n",
    "            self.blue_dot_pos[0] += move_speed\n",
    "        elif action_blue_dot == 2:\n",
    "            self.blue_dot_pos[1] -= move_speed\n",
    "        elif action_blue_dot == 3:\n",
    "            self.blue_dot_pos[1] += move_speed\n",
    "\n",
    "        distance_to_center = np.linalg.norm(\n",
    "            self.blue_dot_pos - np.array([int(self.screen_width / 2), int(self.screen_height / 2)]))\n",
    "        if distance_to_center > int(self.screen_width / 2):\n",
    "            self.blue_dot_pos = self.blue_dot_pos - 2 * (\n",
    "                    self.blue_dot_pos - np.array([int(self.screen_width / 2), int(self.screen_height / 2)]))\n",
    "\n",
    "        food_rewards = []\n",
    "        for i in range(len(self.food_positions)):\n",
    "            food_position = self.food_positions[i]\n",
    "            food_type = self.food_types[i]\n",
    "            food_effect = self.food_effects[i]\n",
    "\n",
    "            distance_to_food = np.linalg.norm(self.blue_dot_pos - food_position)\n",
    "\n",
    "            # Check if the blue dot has collected the food\n",
    "            if distance_to_food < self.blue_dot_radius + self.food_radius:\n",
    "                if food_type == 'pink':\n",
    "                    self.total_reward += 50\n",
    "                    self.blue_dot_health = min(100, self.blue_dot_health + food_effect)\n",
    "                elif food_type == 'green':\n",
    "                    self.total_reward -= 50\n",
    "                    self.blue_dot_health = max(0, self.blue_dot_health - food_effect)\n",
    "                elif food_type == 'yellow':\n",
    "                    self.total_reward += 10  # You can change this if yellow food has a different reward\n",
    "                    self.speed_boost_timer = 10  # Set the speed boost timer to 10 seconds\n",
    "                    move_speed += food_effect\n",
    "\n",
    "                self.eaten_food += 1  # Increment eaten food count\n",
    "\n",
    "                del self.food_positions[i]\n",
    "                del self.food_types[i]\n",
    "                del self.food_effects[i]\n",
    "\n",
    "                break\n",
    "            else:\n",
    "                # Calculate reward based on proximity to the food\n",
    "                reward = 0\n",
    "                if distance_to_food < self.blue_dot_search_radius:\n",
    "                    if food_type == 'pink':\n",
    "                        reward += 0.5  # Increase reward for being close to pink food\n",
    "                    elif food_type == 'green':\n",
    "                        reward -= 0.5  # Decrease reward for being close to green food\n",
    "                    elif food_type == 'yellow':\n",
    "                        reward += 0.2  # Increase reward for being close to yellow food\n",
    "                food_rewards.append(reward)\n",
    "\n",
    "        # Decrement the speed boost timer\n",
    "        if self.speed_boost_timer > 0:\n",
    "            self.speed_boost_timer -= 1\n",
    "            if self.speed_boost_timer == 0:\n",
    "                # Speed boost has expired, reset the speed back to normal\n",
    "                move_speed = 0.9 * self.game_speed\n",
    "\n",
    "        direction = self.blue_dot_pos - np.array([int(self.screen_width / 2), int(self.screen_height / 2)])\n",
    "        direction /= np.linalg.norm(direction)\n",
    "\n",
    "        # Check if it's time to regenerate food\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_food_regeneration_time >= self.food_regeneration_interval:\n",
    "            # Regenerate food items\n",
    "            self.spawn_food()\n",
    "            self.last_food_regeneration_time = current_time  # Update the food regeneration timer\n",
    "\n",
    "        self.blue_dot_pos[0] = np.clip(self.blue_dot_pos[0], 0, self.screen_width / 2)\n",
    "        self.blue_dot_pos[1] = np.clip(self.blue_dot_pos[1], 0, self.screen_height)\n",
    "\n",
    "        observation = np.copy(self.blue_dot_pos)\n",
    "\n",
    "        done = self.blue_dot_health == 0\n",
    "\n",
    "        return observation, self.total_reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            self.screen.fill((93, 97, 140))\n",
    "\n",
    "            pygame.draw.circle(self.screen, (234, 222, 255), (int(self.screen_width / 2), int(self.screen_height / 2)),\n",
    "                               int(self.screen_width / 2) - 1)\n",
    "\n",
    "            pygame.draw.circle(self.screen, (0, 0, 0), (int(self.screen_width / 2), int(self.screen_height / 2)),\n",
    "                               int(self.screen_width / 2), 1)\n",
    "\n",
    "            num_lines = 24\n",
    "            line_length = int(self.screen_width / 2)\n",
    "            line_color = (192, 192, 192)\n",
    "\n",
    "            for angle in range(0, 360, 360 // num_lines):\n",
    "                x_end = int(self.screen_width / 2 + line_length * np.cos(np.radians(angle)))\n",
    "                y_end = int(self.screen_height / 2 + line_length * np.sin(np.radians(angle)))\n",
    "                pygame.draw.line(self.screen, line_color, (int(self.screen_width / 2), int(self.screen_height / 2)),\n",
    "                                 (x_end, y_end), 1)\n",
    "\n",
    "            pygame.draw.circle(self.screen, (141, 144, 226), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])),\n",
    "                               self.blue_dot_radius)\n",
    "\n",
    "            for i in range(len(self.food_positions)):\n",
    "                food_position = self.food_positions[i]\n",
    "                food_type = self.food_types[i]\n",
    "                if food_type == 'pink':\n",
    "                    food_color = (255, 105, 180)\n",
    "                elif food_type == 'green':\n",
    "                    food_color = (0, 128, 0)\n",
    "                elif food_type == 'yellow':\n",
    "                    food_color = (255, 255, 0)\n",
    "                pygame.draw.circle(self.screen, food_color, (int(food_position[0]), int(food_position[1])),\n",
    "                                   self.food_radius)\n",
    "\n",
    "            pygame.draw.circle(self.screen, (0, 0, 255), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])),\n",
    "                               self.blue_dot_search_radius, 1)\n",
    "\n",
    "            self.display_total_reward()\n",
    "\n",
    "            pygame.display.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DotEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Models', 'DQN_Model')\n",
    "log_path = os.path.join('Training', 'DQN_Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env=env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\dqn\\dqn2\\updated.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m500000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect_rollouts(\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv,\n\u001b[0;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_freq,\n\u001b[0;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_noise,\n\u001b[0;32m    316\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_starts,\n\u001b[0;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer,\n\u001b[0;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    541\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    543\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    547\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\dqn\\dqn2\\updated.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated.ipynb#X10sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m food_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfood_types[i]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated.ipynb#X10sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m food_effect \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfood_effects[i]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated.ipynb#X10sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m distance_to_food \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblue_dot_pos \u001b[39m-\u001b[39m food_position)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated.ipynb#X10sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39m# Check if the blue dot has collected the food\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated.ipynb#X10sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39mif\u001b[39;00m distance_to_food \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblue_dot_radius \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfood_radius:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2511\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2509\u001b[0m     sqnorm \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mdot(x_real) \u001b[39m+\u001b[39m x_imag\u001b[39m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2511\u001b[0m     sqnorm \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdot(x)\n\u001b[0;32m   2512\u001b[0m ret \u001b[39m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} Std reward: {std_reward}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
