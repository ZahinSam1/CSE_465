{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Dict, Box, MultiDiscrete, Tuple\n",
    "\n",
    "from Agents.agent import Agent\n",
    "# from Agents.fov_points import get_fov_points\n",
    "# from Agents.overlap_detection import detect_overlapping_points\n",
    "from Agents.RayCast import get_fov_rays\n",
    "from Constants.constants import WHITE, RED, BLUE, SCREEN_WIDTH, SCREEN_HEIGHT, WALLS, FOV_RADIUS, LEVEL_1_WALLS, LEVEL_2_WALLS, LEVEL_3_WALLS\n",
    "from Walls.collision_detection import detect_collision\n",
    "from Walls.wall_class import Walls\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALLS2 = LEVEL_2_WALLS\n",
    "class GameEnv(Env):\n",
    "    metadata = {\"render_modes\" : [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = SCREEN_WIDTH\n",
    "        self.screen_height = SCREEN_HEIGHT\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.observation_space = Dict({\n",
    "            \"predator_position\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "                                    high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                    dtype=np.float32),\n",
    "            \"predator_angle\": Discrete(360),\n",
    "            # to send only the points for which he has to cross\n",
    "            \"destination_coordinates\": Box(low=np.array([0, 0], dtype=np.float32),\n",
    "                                    high=np.array([self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                    dtype=np.float32),\n",
    "            \n",
    "        })\n",
    "\n",
    "        self.action_space = Discrete(3)\n",
    "        # 3 for \n",
    "        # rotate clockwise, anti-clock\n",
    "        # move front\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_agent = Agent('predator', 0)\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        self.obs = None\n",
    "\n",
    "        # start the tick timer\n",
    "        self.start_time = 0\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "        # for the wall initializations\n",
    "        self.wall = Walls(pygame)\n",
    "        self.walls = None\n",
    "\n",
    "    def flatten_list(self, nested_list):\n",
    "        flattened_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list) :\n",
    "                flattened_list.extend(self.flatten_list(item))\n",
    "            else:\n",
    "                flattened_list.append(item)\n",
    "        return flattened_list\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        observation = {\n",
    "            \"predator_position\": self.predator_agent.current_position,\n",
    "            \"predator_angle\": self.predator_agent.angle,\n",
    "            \"destination_coordinates\": None,  # need to send a np.array for the goal to reach,\n",
    "        }\n",
    "\n",
    "        # print(observation)\n",
    "        return observation\n",
    "    \n",
    "    # to capture all the info\n",
    "    def _get_info(self):\n",
    "        \n",
    "        info = {\n",
    "            \"goal_seen\": self.goal_seen,\n",
    "            \"distance\": 450,\n",
    "            \"vision_blocked\": not self.goal_seen,\n",
    "        }\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "    # def _max_right(self):\n",
    "    #     max_right = 0\n",
    "\n",
    "    #     for wall in self.walls:\n",
    "    #         if wall.right > max_right:\n",
    "    #             max_right = wall.right\n",
    "        \n",
    "    #     return max_right\n",
    "\n",
    "    def get_reward(self, reward):\n",
    "        CURVE = -0.03\n",
    "        ASCEND = 0.02\n",
    "        CLAMP = 10\n",
    "\n",
    "        reward = reward\n",
    "        goal_coordinate = 0\n",
    "        agent_pos = self.predator_agent.current_position\n",
    "        if seen:\n",
    "            direction = goal_coordinate - agent_pos\n",
    "            distance = np.linalg.norm(direction)\n",
    "            reward += ASCEND * np.exp(CURVE * distance) - CLAMP\n",
    "\n",
    "        if agent_pos[0] > goal_coordinate[0] and agent_pos[1] < goal_coordinate[1]:\n",
    "            done = True\n",
    "            reward = 200\n",
    "\n",
    "        reward += 0.0001\n",
    "\n",
    "        print(f'direction: {direction}, distance:{distance}, reward: {reward}')\n",
    "        return reward\n",
    "\n",
    "\n",
    "    # the usual reset function\n",
    "    def reset(self, seed=None, option=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "        self.wall.clear_walls()\n",
    "        self.walls = self.wall.make_wall(WALLS2)\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        # for predator in self.predator_agents:\n",
    "        self.predator_agent.agent_reset(width=self.screen_width, height=self.screen_height, walls=self.walls)\n",
    "\n",
    "        # all the variable values inside the observation space needs to be sent inside the observation variable\n",
    "        # for this level purpose we decided to add the dictionary observation\n",
    "        # set the observation to a dictionary\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # initializing the return variables\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "\n",
    "        elapsed_time = current_time - self.start_time\n",
    "\n",
    "        self.predator_agent.step_update(action, range_x=self.screen_width, range_y=self.screen_height)\n",
    "        self.predator_agent = detect_collision(self.predator_agent, self.walls)\n",
    "\n",
    "        # observation needs to be set a dictionary\n",
    "\n",
    "        self.total_steps += 1\n",
    "        reward = self.get_reward(reward)\n",
    "        \n",
    "\n",
    "        # if self.predator_agent.current_position[0] > self.walls[1].right:\n",
    "        #     reward += 100\n",
    "        # # for wall in self.walls:\n",
    "        # if self.predator_agent.current_position[0] > self._max_right():\n",
    "        #     reward += 200\n",
    "        #     done = True\n",
    "\n",
    "        if elapsed_time >= self.total_running_time + 10:\n",
    "            reward -= 100\n",
    "            truncated = True\n",
    "        \"\"\"\n",
    "        here lies the most important task\n",
    "        handling the rewards\n",
    "        \"\"\"\n",
    "        \n",
    "        # getting observation and info\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        self.predator_total_reward = reward\n",
    "        self.obs = observation\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'rgb_array':\n",
    "            self._render_frame()\n",
    "\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            pygame.font.init()\n",
    "\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        screen.fill(WHITE)\n",
    "        \n",
    "        predator = self.predator_agent\n",
    "        pygame.draw.circle(screen, RED, predator.center, predator.radius)\n",
    "        pygame.draw.line(screen, RED, predator.center, predator.draw_direction_end, 5)\n",
    "\n",
    "        mid_point = 0\n",
    "\n",
    "        if self.predator_agent.current_position[0] < self.walls[0].midbottom[0]:\n",
    "            mid_point = (np.array(self.walls[0].midbottom, dtype=np.float32)  + np.array(self.walls[1].midtop, dtype=np.float32)) / 2\n",
    "            # print(f'mid point: {mid_point}')\n",
    "            direction = mid_point - self.predator_agent.current_position\n",
    "        \n",
    "        if self.walls[0].right < self.predator_agent.current_position[0] < self.walls[2].midbottom[0]:\n",
    "            mid_point = (np.array(self.walls[2].midbottom, dtype=np.float32) + np.array(self.walls[3].midtop, dtype=np.float32)) / 2\n",
    "            direction = mid_point - self.predator_agent.current_position\n",
    "        mid_point = (int(mid_point[0]), int(mid_point[1]))\n",
    "        pygame.draw.line(screen, RED, predator.center, mid_point, 2)\n",
    "\n",
    "        for key, wall in WALLS2.items():\n",
    "            pygame.draw.rect(screen, BLUE, (wall['x'], wall['y'], wall['width'], wall['height']))\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "\n",
    "            font = pygame.font.Font(None, 18)\n",
    "        \n",
    "            text_surface = font.render(f\"Reward: {self.predator_total_reward: .5f} \", True, (0, 0, 0))\n",
    "\n",
    "            text_rect = text_surface.get_rect()\n",
    "\n",
    "            text_rect.center = (self.screen_width - 200, 10)\n",
    "            \n",
    "            screen.blit(text_surface, text_rect)\n",
    "            self.window.blit(screen, screen.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # this part is to fix the fps of rendering\n",
    "            # self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        \n",
    "        else:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.font.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "env.predator_agent.current_position = np.array([30, 300], dtype=np.float32)\n",
    "done = False\n",
    "traunc = False\n",
    "while not traunc:\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, traunc, _ = env.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.vec_env import VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs', 'Level_02_DQN')\n",
    "baseline_path = os.path.join('Training', 'Models', 'Level_02_DQN')\n",
    "callback_basealine_path = os.path.join('Training', 'Models', 'callback_Level_02_DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = os.path.join('Training', 'Models', 'callback_Level_01_DQN', 'best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=202, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                                callback_on_new_best=stop_callback, \n",
    "                                eval_freq=100000, \n",
    "                                best_model_save_path=baseline_path, \n",
    "                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MultiInputPolicy', env, verbose=1, tensorboard_log=log_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\Level_02_DQN\\DQN_6\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.78e+03 |\n",
      "|    ep_rew_mean      | -71.4    |\n",
      "|    exploration_rate | 0.969    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 288      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total_timesteps  | 23120    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.58e+03 |\n",
      "|    ep_rew_mean      | -76.6    |\n",
      "|    exploration_rate | 0.939    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 279      |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total_timesteps  | 44676    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.33e+03 |\n",
      "|    ep_rew_mean      | -73.1    |\n",
      "|    exploration_rate | 0.913    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 266      |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total_timesteps  | 63938    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000744 |\n",
      "|    n_updates        | 3484     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 5.09e+03 |\n",
      "|    ep_rew_mean      | -74      |\n",
      "|    exploration_rate | 0.889    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 254      |\n",
      "|    time_elapsed     | 320      |\n",
      "|    total_timesteps  | 81518    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.35e-05 |\n",
      "|    n_updates        | 7879     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.94e+03 |\n",
      "|    ep_rew_mean      | -75.3    |\n",
      "|    exploration_rate | 0.866    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 246      |\n",
      "|    time_elapsed     | 400      |\n",
      "|    total_timesteps  | 98773    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.51e-05 |\n",
      "|    n_updates        | 12193    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-83.92 +/- 0.18\n",
      "Episode length: 4118.60 +/- 45.40\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.12e+03 |\n",
      "|    mean_reward      | -83.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.864    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.45e-06 |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.88e+03 |\n",
      "|    ep_rew_mean      | -75.9    |\n",
      "|    exploration_rate | 0.841    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 200      |\n",
      "|    time_elapsed     | 585      |\n",
      "|    total_timesteps  | 117233   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.09e-05 |\n",
      "|    n_updates        | 16808    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.79e+03 |\n",
      "|    ep_rew_mean      | -76.9    |\n",
      "|    exploration_rate | 0.818    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 201      |\n",
      "|    time_elapsed     | 665      |\n",
      "|    total_timesteps  | 134104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.71e-05 |\n",
      "|    n_updates        | 21025    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.72e+03 |\n",
      "|    ep_rew_mean      | -77.8    |\n",
      "|    exploration_rate | 0.795    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 202      |\n",
      "|    time_elapsed     | 745      |\n",
      "|    total_timesteps  | 151021   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.79e-05 |\n",
      "|    n_updates        | 25255    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.66e+03 |\n",
      "|    ep_rew_mean      | -79      |\n",
      "|    exploration_rate | 0.772    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 825      |\n",
      "|    total_timesteps  | 167884   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.21e-05 |\n",
      "|    n_updates        | 29470    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.61e+03 |\n",
      "|    ep_rew_mean      | -79.7    |\n",
      "|    exploration_rate | 0.75     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 203      |\n",
      "|    time_elapsed     | 906      |\n",
      "|    total_timesteps  | 184359   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.58e-06 |\n",
      "|    n_updates        | 33589    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-82.97 +/- 0.16\n",
      "Episode length: 4252.40 +/- 40.19\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.25e+03 |\n",
      "|    mean_reward      | -83      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.729    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.36e-05 |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.64e+03 |\n",
      "|    ep_rew_mean      | -80.1    |\n",
      "|    exploration_rate | 0.723    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 185      |\n",
      "|    time_elapsed     | 1102     |\n",
      "|    total_timesteps  | 204117   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.97e-05 |\n",
      "|    n_updates        | 38529    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.59e+03 |\n",
      "|    ep_rew_mean      | -80.8    |\n",
      "|    exploration_rate | 0.701    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 186      |\n",
      "|    time_elapsed     | 1182     |\n",
      "|    total_timesteps  | 220501   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.54e-05 |\n",
      "|    n_updates        | 42625    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.55e+03 |\n",
      "|    ep_rew_mean      | -81.5    |\n",
      "|    exploration_rate | 0.679    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 187      |\n",
      "|    time_elapsed     | 1262     |\n",
      "|    total_timesteps  | 236738   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.23e-05 |\n",
      "|    n_updates        | 46684    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52e+03 |\n",
      "|    ep_rew_mean      | -82.2    |\n",
      "|    exploration_rate | 0.656    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 188      |\n",
      "|    time_elapsed     | 1342     |\n",
      "|    total_timesteps  | 253117   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.82e-05 |\n",
      "|    n_updates        | 50779    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.49e+03 |\n",
      "|    ep_rew_mean      | -82.7    |\n",
      "|    exploration_rate | 0.634    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 189      |\n",
      "|    time_elapsed     | 1422     |\n",
      "|    total_timesteps  | 269448   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51e-05 |\n",
      "|    n_updates        | 54861    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.46e+03 |\n",
      "|    ep_rew_mean      | -83.1    |\n",
      "|    exploration_rate | 0.612    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 190      |\n",
      "|    time_elapsed     | 1502     |\n",
      "|    total_timesteps  | 285690   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000602 |\n",
      "|    n_updates        | 58922    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-83.98 +/- 0.12\n",
      "Episode length: 4295.20 +/- 31.98\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.3e+03  |\n",
      "|    mean_reward      | -84      |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.593    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.3e-05  |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.47e+03 |\n",
      "|    ep_rew_mean      | -83.5    |\n",
      "|    exploration_rate | 0.587    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 179      |\n",
      "|    time_elapsed     | 1694     |\n",
      "|    total_timesteps  | 303985   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.7e-06  |\n",
      "|    n_updates        | 63496    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.45e+03 |\n",
      "|    ep_rew_mean      | -83.9    |\n",
      "|    exploration_rate | 0.566    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 180      |\n",
      "|    time_elapsed     | 1774     |\n",
      "|    total_timesteps  | 320089   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.33e-06 |\n",
      "|    n_updates        | 67522    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.42e+03 |\n",
      "|    ep_rew_mean      | -84.3    |\n",
      "|    exploration_rate | 0.544    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 1854     |\n",
      "|    total_timesteps  | 336030   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.12     |\n",
      "|    n_updates        | 71507    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.4e+03  |\n",
      "|    ep_rew_mean      | -84.6    |\n",
      "|    exploration_rate | 0.522    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 181      |\n",
      "|    time_elapsed     | 1934     |\n",
      "|    total_timesteps  | 351882   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.43e-05 |\n",
      "|    n_updates        | 75470    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.38e+03 |\n",
      "|    ep_rew_mean      | -84.9    |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 2014     |\n",
      "|    total_timesteps  | 367647   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.22e-06 |\n",
      "|    n_updates        | 79411    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.35e+03 |\n",
      "|    ep_rew_mean      | -85.2    |\n",
      "|    exploration_rate | 0.48     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 182      |\n",
      "|    time_elapsed     | 2094     |\n",
      "|    total_timesteps  | 383225   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.59e-06 |\n",
      "|    n_updates        | 83306    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33e+03 |\n",
      "|    ep_rew_mean      | -85.4    |\n",
      "|    exploration_rate | 0.459    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 183      |\n",
      "|    time_elapsed     | 2174     |\n",
      "|    total_timesteps  | 398630   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.01e-06 |\n",
      "|    n_updates        | 87157    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-90.84 +/- 0.08\n",
      "Episode length: 4282.80 +/- 40.33\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.28e+03 |\n",
      "|    mean_reward      | -90.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.457    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.03e-05 |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.33e+03 |\n",
      "|    ep_rew_mean      | -85.6    |\n",
      "|    exploration_rate | 0.436    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 175      |\n",
      "|    time_elapsed     | 2361     |\n",
      "|    total_timesteps  | 415550   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000152 |\n",
      "|    n_updates        | 91387    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.31e+03 |\n",
      "|    ep_rew_mean      | -85.8    |\n",
      "|    exploration_rate | 0.415    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 176      |\n",
      "|    time_elapsed     | 2441     |\n",
      "|    total_timesteps  | 430691   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.93e-06 |\n",
      "|    n_updates        | 95172    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.23e+03 |\n",
      "|    ep_rew_mean      | -86.5    |\n",
      "|    exploration_rate | 0.395    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 176      |\n",
      "|    time_elapsed     | 2521     |\n",
      "|    total_timesteps  | 445965   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.87e-06 |\n",
      "|    n_updates        | 98991    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.16e+03 |\n",
      "|    ep_rew_mean      | -86.8    |\n",
      "|    exploration_rate | 0.374    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 177      |\n",
      "|    time_elapsed     | 2601     |\n",
      "|    total_timesteps  | 461161   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.64e-05 |\n",
      "|    n_updates        | 102790   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.12e+03 |\n",
      "|    ep_rew_mean      | -87.8    |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 177      |\n",
      "|    time_elapsed     | 2681     |\n",
      "|    total_timesteps  | 476254   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.32e-06 |\n",
      "|    n_updates        | 106563   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.1e+03  |\n",
      "|    ep_rew_mean      | -88.4    |\n",
      "|    exploration_rate | 0.333    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 177      |\n",
      "|    time_elapsed     | 2761     |\n",
      "|    total_timesteps  | 491195   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.53e-06 |\n",
      "|    n_updates        | 110298   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-81.28 +/- 0.22\n",
      "Episode length: 4265.60 +/- 49.81\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 4.27e+03 |\n",
      "|    mean_reward      | -81.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.321    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.68e-06 |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.09e+03 |\n",
      "|    ep_rew_mean      | -88.7    |\n",
      "|    exploration_rate | 0.311    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 172      |\n",
      "|    time_elapsed     | 2948     |\n",
      "|    total_timesteps  | 507454   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.17e-06 |\n",
      "|    n_updates        | 114363   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.05e+03 |\n",
      "|    ep_rew_mean      | -89.2    |\n",
      "|    exploration_rate | 0.291    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 172      |\n",
      "|    time_elapsed     | 3028     |\n",
      "|    total_timesteps  | 522133   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.21e-06 |\n",
      "|    n_updates        | 118033   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.02e+03 |\n",
      "|    ep_rew_mean      | -89.6    |\n",
      "|    exploration_rate | 0.272    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 172      |\n",
      "|    time_elapsed     | 3108     |\n",
      "|    total_timesteps  | 536449   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.07e-06 |\n",
      "|    n_updates        | 121612   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -89.9    |\n",
      "|    exploration_rate | 0.252    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 172      |\n",
      "|    time_elapsed     | 3188     |\n",
      "|    total_timesteps  | 551030   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.25e-06 |\n",
      "|    n_updates        | 125257   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\CSE_465\\test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m7000000\u001b[39m, callback\u001b[39m=\u001b[39meval_callback)\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect_rollouts(\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv,\n\u001b[0;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_freq,\n\u001b[0;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_noise,\n\u001b[0;32m    316\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_starts,\n\u001b[0;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer,\n\u001b[0;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    541\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    543\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    547\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\jahin\\anaconda3\\envs\\myenv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\CSE_465\\test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=243'>244</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs \u001b[39m=\u001b[39m observation\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=245'>246</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=246'>247</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_frame()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m \u001b[39mreturn\u001b[39;00m observation, reward, done, truncated, info\n",
      "\u001b[1;32mc:\\jupyter\\Reinforcement Learning\\CSE_465\\test.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=286'>287</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow\u001b[39m.\u001b[39mblit(screen, screen\u001b[39m.\u001b[39mget_rect())\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=287'>288</a>\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=288'>289</a>\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=290'>291</a>\u001b[0m     \u001b[39m# this part is to fix the fps of rendering\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=291'>292</a>\u001b[0m     \u001b[39m# self.clock.tick(self.metadata[\"render_fps\"])\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=292'>293</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=293'>294</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=294'>295</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=295'>296</a>\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(screen)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/jupyter/Reinforcement%20Learning/CSE_465/test.ipynb#X14sZmlsZQ%3D%3D?line=296'>297</a>\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "model.learn(total_timesteps=7000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(baseline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(baseline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv(render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206.93253619526513, 0.0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSTACLES = {\n",
    "    'walls': {\n",
    "        1: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'},\n",
    "        2: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'},\n",
    "        3: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'},\n",
    "        4: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'},\n",
    "        5: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 2, 'access': 'bottom_top', 'orientation': 'horizontal'},\n",
    "        6: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 1},\n",
    "        7: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 4},\n",
    "        8: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 5},\n",
    "        9: {'x': SCREEN_WIDTH // 2 - 25, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 4},\n",
    "    },\n",
    "    'gates': {\n",
    "        1: {'type': 'left_right', 'left': 5, 'right':6, 'orientiation': 'vertical'},\n",
    "        2: {'type': 'bottom_top', 'top': 3, 'right':4, 'orientiation': 'horizontal'},\n",
    "    }\n",
    "\n",
    "}\n",
    "# direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'})\n",
      "(2, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'})\n",
      "(3, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'})\n",
      "(4, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'b_w'})\n",
      "(5, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 2, 'access': 'bottom_top', 'orientation': 'horizontal'})\n",
      "(6, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 1})\n",
      "(7, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 4})\n",
      "(8, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 5})\n",
      "(9, {'x': 375, 'y': 0, 'width': 50, 'height': 50, 'name': 'o_w', 'gate_with': 4})\n"
     ]
    }
   ],
   "source": [
    "for objects in OBSTACLES.items():\n",
    "    for walls in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
