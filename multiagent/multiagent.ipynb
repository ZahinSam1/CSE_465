{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_name, agent_index):\n",
    "        self.index = agent_index\n",
    "        self.agent = agent_name\n",
    "        self.health = None\n",
    "        self.isHit = False\n",
    "        self.move = True\n",
    "        self.movement_speed = 1.00\n",
    "        self.previous_position = np.array([0, 0], dtype=np.float32)\n",
    "        self.current_position = None\n",
    "        self.same_position = False\n",
    "        self.current_step = 0\n",
    "        self.action = None\n",
    "        pass\n",
    "\n",
    "    def agent_action(self, action):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def agent_reset(self, width, height):\n",
    "        padding = 30\n",
    "        self.current_position = np.array(\n",
    "            [np.random.uniform(30, width - padding), np.random.uniform(30, width - padding)], dtype=np.float32)\n",
    "\n",
    "    def step_update(self, action, range_x, range_y):\n",
    "\n",
    "        if action == 0:\n",
    "            self.current_position[0] -= self.movement_speed\n",
    "        elif action == 1:\n",
    "            self.current_position[0] += self.movement_speed\n",
    "        elif action == 2:\n",
    "            self.current_position[1] -= self.movement_speed\n",
    "        elif action == 3:\n",
    "            self.current_position[1] += self.movement_speed\n",
    "        \n",
    "        self.current_position[0] = np.clip(self.current_position[0], 0, range_x)\n",
    "        self.current_position[1] = np.clip(self.current_position[1], 0, range_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, Box, MultiDiscrete\n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.001235008239746\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, screen_width=400, screen_height=400, render_mode='human'):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # defining the observation and action spaces for all the agents\n",
    "        \n",
    "        self.observation_space = Box(low=np.array([0, 0, 0, 0], dtype=np.float32),\n",
    "                                    high=np.array([self.screen_width, self.screen_height, self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "        # the pygame window should be initialized in the render function\n",
    "\n",
    "        # setting the total number of agent\n",
    "        \n",
    "        self.number_of_prey = 1\n",
    "        self.number_of_predator = 1\n",
    "        self.prey_agent = None\n",
    "        self.predator_agent = None\n",
    "        self.predator_i_position = None\n",
    "        self.initial_distance = None\n",
    "        self.current_distance = None\n",
    "        self.predator_total_reward = 0\n",
    "        self.number_of_agents = self.number_of_prey + self.number_of_prey\n",
    "\n",
    "        # defining the action space based on total number of predator and prey\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "\n",
    "        # setting the total number of obstacles\n",
    "        self.total_obstacles = None\n",
    "\n",
    "        # keeping a counter to save the total steps\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # initializing the pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # setting the screen size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Multi Agent Environment(simple)')\n",
    "        \n",
    "        # keep the track of time of the rendering\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        # *it is set in milisec format (time is sec is time/1000)\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        # start the tick timer\n",
    "        self.start_time = 0\n",
    "        # print(f'start  time: {self.start_time}')\n",
    "\n",
    "        # initializing the font\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 18)\n",
    "\n",
    "    # this function rerturns the value of the action into 2 digits \n",
    "    # if the action_space.sample() gives 1 digit number\n",
    "    # * if  the number is 3 it will return 03 \n",
    "    # * if  the number is 14 then it will return 14\n",
    "    def expand_action_digit(self, action):\n",
    "\n",
    "        # this basically checks the number if it has 1 then fills the rest with 0\n",
    "        # if the number is 2 digits then it stays the same\n",
    "        action = str(action).zfill(2)\n",
    "        prey_action = int(action[0]) % 4\n",
    "        predator_action = int(action[1]) % 4\n",
    "        return prey_action, predator_action\n",
    "        \n",
    "\n",
    "    # this method will initialize the number of agents\n",
    "    # ! this must be called from outside\n",
    "    def agent_init(self):\n",
    "\n",
    "        prey_agents = Agent('prey', 0)\n",
    "\n",
    "        predator_agents = Agent('predator', 0)\n",
    "\n",
    "        self.prey_agent = prey_agents\n",
    "        self.predator_agent = predator_agents\n",
    "        \n",
    "\n",
    "    # this function is used to explicitly set the number of agents\n",
    "    # ! this needs to be called from outside\n",
    "    def set_agent_number(self, prey_number, predator_number):\n",
    "        self.number_of_predator = predator_number\n",
    "        self.number_of_prey = prey_number\n",
    "\n",
    "    # the usual reset function\n",
    "    def reset(self, seed=0):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        self.agent_init()\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        prey = self.prey_agent\n",
    "        predator = self.predator_agent\n",
    "\n",
    "        # for prey in self.prey_agents:\n",
    "        prey.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "        # observation.append([prey.index, prey.agent, prey.current_position])\n",
    "\n",
    "        # for predator in self.predator_agents:\n",
    "        predator.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "        # observation.append([predator.index, predator.agent, predator.current_position])\n",
    "        \n",
    "        # setting the predator and prey to their initial position\n",
    "        self.prey_agent = prey\n",
    "        self.predator_agent = predator\n",
    "\n",
    "        # setting the initial position of predator for reward\n",
    "        self.predator_i_position = self.predator_agent.current_position\n",
    "        \n",
    "        # calculating the initial distance of 2 agents\n",
    "        direction = self.predator_agent.current_position - self.prey_agent.current_position\n",
    "        self.initial_distance = np.linalg.norm(direction)\n",
    "\n",
    "        # observation :\n",
    "        # all the variable values inside the obsercation space needs to be sent inside the observation variable\n",
    "        observation = np.concatenate([self.prey_agent.current_position, self.predator_agent.current_position])\n",
    "        return observation, seed\n",
    "\n",
    "    # the step function\n",
    "    # this function is called for every timesteps\n",
    "    # this function updates the actions or states of agents in the env\n",
    "    # this function is called default by the algorithms of all sorts\n",
    "    # * it returns observation, reward, done, truncated, info\n",
    "    # * any game policy change can be done here\n",
    "    # * reward must be set here\n",
    "    def step(self, action):\n",
    "        \n",
    "        # initializing the return variables\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "        # print(f'current time: {current_time}')\n",
    "        # when ever the step is starting set the start time\n",
    "        # if self.total_steps == 0:\n",
    "        #     self.start_time = pygame.time.get_ticks()\n",
    "        # else:\n",
    "        #     current_time = pygame.time.get_ticks()\n",
    "\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        # print(f'elapsed time: {elapsed_time}')\n",
    "        # handles the pygame window event when closing\n",
    "        # !if the window still crashes pygame.event needs to be managed properly\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                done = True\n",
    "                pygame.quit()\n",
    "        \n",
    "        # print(f'action: {action}')\n",
    "        # *the actions are split as required\n",
    "        # prey_action, predator_action = self.expand_action_digit(action)\n",
    "        predator_action = action\n",
    "        predator = self.predator_agent\n",
    "        # print(f'prey: {prey_action}, predator: {predator_action}')\n",
    "                \n",
    "\n",
    "        # print(f'predator_{predator.index} = action:{action} current_position: {predator.current_position}')\n",
    "        predator.step_update(action=predator_action, range_x=self.screen_width - 10, range_y=self.screen_height - 10)\n",
    "        # print(f'predator_{predator.index}: new_position: {predator.current_position}')\n",
    "            \n",
    "        # !observation.append({'index': predator.index, 'name': predator.agent, 'position': predator.current_position})\n",
    "        # observation = self.predator_agent.current_position\n",
    "        observation = np.concatenate([self.prey_agent.current_position, self.predator_agent.current_position])\n",
    "\n",
    "        # print(f'observation: {self.predator_agent.current_position}')\n",
    "        self.total_steps += 1\n",
    "\n",
    "        direction = self.predator_agent.current_position - self.prey_agent.current_position\n",
    "\n",
    "        # Calculate the distance between the centers of the two dots\n",
    "        distance_between_centers = np.linalg.norm(direction)\n",
    "        self.current_distance = distance_between_centers\n",
    "\n",
    "        # check everystep if the distance of two agents are:\n",
    "        # greater than  initial distance?  \n",
    "        # yes: -reward\n",
    "        # no: +reward\n",
    "\n",
    "        # if distance_between_centers > self.initial_distance:\n",
    "        #     reward -= 0.06\n",
    "        # else:\n",
    "        #     reward += 0.01\n",
    "\n",
    "        k = 0.015\n",
    "        if distance_between_centers > self.initial_distance:\n",
    "            reward -= 0.02 * np.exp(-k * (distance_between_centers + 10)) - 0.007\n",
    "        else:\n",
    "            reward += 0.02 * np.exp(-k * (distance_between_centers + 10)) - 0.0005\n",
    "\n",
    "        # if distance_between_centers > self.initial_distance:\n",
    "        #     reward -= 2 * np.exp(-distance_between_centers * 0.5) - 0.5\n",
    "        # else:\n",
    "        #     reward += 2 * np.exp(-distance_between_centers * 0.5) - 0.5\n",
    "            \n",
    "\n",
    "        # Check if there is a collision (distance <= sum of radii)\n",
    "        if elapsed_time <= self.total_running_time:\n",
    "            if distance_between_centers <= 20:\n",
    "                reward += 40\n",
    "                done = True\n",
    "                    # pygame.quit()\n",
    "                    # self.close()\n",
    "            # if self.total_steps == 30000:\n",
    "            #     done = True\n",
    "        else:\n",
    "            done = True\n",
    "            reward -= 20\n",
    "                # pygame.quit()\n",
    "                # self.close()\n",
    "\n",
    "        # print(self.total_steps)\n",
    "        self.render()\n",
    "        # it will update the total reward everystep\n",
    "        self.predator_total_reward = reward\n",
    "\n",
    "        return observation, reward, done, _, info\n",
    "        \n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            screen = self.screen\n",
    "\n",
    "            # clear screen\n",
    "            screen.fill((255, 255, 255))\n",
    "            prey = self.prey_agent\n",
    "            pos_x, pos_y = prey.current_position\n",
    "            prey_radius = 10\n",
    "            pygame.draw.circle(screen, (0, 0, 255), (int(pos_x), int(pos_y)), prey_radius)\n",
    "\n",
    "            predator = self.predator_agent\n",
    "            pos_x, pos_y = predator.current_position\n",
    "            predator_radius = 10\n",
    "\n",
    "            pygame.draw.circle(screen, (255, 0, 0), (int(pos_x), int(pos_y)), predator_radius)\n",
    "\n",
    "            text_surface = self.font.render(f\"Reward: {self.predator_total_reward: .5f} initial distance: {self.initial_distance: .2f} current_distance:{self.current_distance: .2f}\", True, (0, 0, 0))\n",
    "\n",
    "            text_rect = text_surface.get_rect()\n",
    "\n",
    "            text_rect.center = (self.screen_width - 200, 10)\n",
    "\n",
    "            self.screen.blit(text_surface, text_rect)\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "env.reset()\n",
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: -19.99950006828541\n",
      "Number of steps: 39912\n",
      "total reward: -20.000499996427052\n",
      "Number of steps: 41978\n",
      "total reward: -19.999500073152387\n",
      "Number of steps: 39937\n",
      "total reward: 40.01541480837144\n",
      "Number of steps: 2008\n",
      "total reward: 40.01520710697384\n",
      "Number of steps: 7849\n"
     ]
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "for i in range(0, 5):\n",
    "    done = False\n",
    "\n",
    "    # env.agent_init()\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        # total_reward += reward\n",
    "    print(f'total reward: {reward}')\n",
    "    print(f'Number of steps: {env.total_steps}')\n",
    "        # env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.callbacks import BaseCallback\n",
    "# import os\n",
    "\n",
    "# class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "#     def __init__(self, check_freq, log_dir, verbose=1):\n",
    "#         super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "#         self.check_freq = check_freq  # How often to check for best reward\n",
    "#         self.log_dir = log_dir  # Directory to save the best model\n",
    "#         self.best_mean_reward = -float(\"inf\")\n",
    "\n",
    "#     def _init_callback(self) -> None:\n",
    "#         # Create the log directory if it doesn't exist\n",
    "#         os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "#     def _on_step(self) -> bool:\n",
    "#         if self.n_calls % self.check_freq == 0:\n",
    "#             # Evaluate the model's performance\n",
    "#             mean_reward = self.eval_model()\n",
    "#             if mean_reward > self.best_mean_reward:\n",
    "#                 # If the current performance is better, save the model\n",
    "#                 self.best_mean_reward = mean_reward\n",
    "#                 self.model.save(os.path.join(self.log_dir, \"best_model\"))\n",
    "\n",
    "#     def eval_model(self):\n",
    "#         # Perform evaluation and return the mean reward\n",
    "#         # You can adapt this part based on how you evaluate the model\n",
    "#         # In this example, it assumes you have an environment and a trained model\n",
    "#         mean_reward = 0\n",
    "#         num_episodes = 10\n",
    "#         for _ in range(num_episodes):\n",
    "#             obs = self.eval_env.reset()\n",
    "#             episode_reward = 0\n",
    "#             done = False\n",
    "#             while not done:\n",
    "#                 action, _ = self.model.predict(obs, deterministic=True)\n",
    "#                 obs, reward, done, _ = self.eval_env.step(action)\n",
    "#                 episode_reward += reward\n",
    "#             mean_reward += episode_reward\n",
    "#         mean_reward /= num_episodes\n",
    "#         return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os\n",
    "\n",
    "log_dir = \".\\Training\\Logs\"  # Change this to your desired directory\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # Create a callback to log data to TensorBoard\n",
    "# callback = EvalCallback(\n",
    "#     eval_env=env,\n",
    "#     callback_on_new_best=log_dir,\n",
    "#     n_eval_episodes=10,  # Adjust as needed\n",
    "#     best_model_save_path=log_dir,\n",
    "#     log_path=log_dir,\n",
    "# )\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([119.338005, 246.29778 , 206.15166 , 144.73596 ], dtype=float32), 0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training\\Logs\\PPO_10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1278 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 983         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019885559 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0485     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.00349     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 941         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012191961 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011837886 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0485     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00652    |\n",
      "|    value_loss           | 0.000894    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.54e+03    |\n",
      "|    ep_rew_mean          | 21.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 914         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014427982 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.54e+03    |\n",
      "|    ep_rew_mean          | 21.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 908         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003895292 |\n",
      "|    clip_fraction        | 0.0655      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.015       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    value_loss           | 1.41        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.54e+03    |\n",
      "|    ep_rew_mean          | 21.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 900         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011787803 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00723    |\n",
      "|    value_loss           | 0.00382     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.54e+03    |\n",
      "|    ep_rew_mean          | 21.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013646867 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00166     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00574    |\n",
      "|    value_loss           | 0.00187     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.32e+03    |\n",
      "|    ep_rew_mean          | 26.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011267874 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0347     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    value_loss           | 0.000415    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.32e+03    |\n",
      "|    ep_rew_mean          | 26.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007354783 |\n",
      "|    clip_fraction        | 0.0569      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0563      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.381       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00933    |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.32e+03    |\n",
      "|    ep_rew_mean          | 26.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011734873 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000332   |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 0.000245    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.32e+03    |\n",
      "|    ep_rew_mean          | 26.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 864         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015614312 |\n",
      "|    clip_fraction        | 0.379       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.328       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0421     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    value_loss           | 0.00012     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.35e+03    |\n",
      "|    ep_rew_mean          | 28.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013906329 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.028      |\n",
      "|    value_loss           | 0.00022     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.35e+03    |\n",
      "|    ep_rew_mean          | 28.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012295071 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0329     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 1.7         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.35e+03    |\n",
      "|    ep_rew_mean          | 28.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008339829 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -0.0805     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00761     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000368   |\n",
      "|    value_loss           | 0.000248    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.68e+03    |\n",
      "|    ep_rew_mean          | 18.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012621362 |\n",
      "|    clip_fraction        | 0.0791      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.000768    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0016      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 0.000216    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.68e+03   |\n",
      "|    ep_rew_mean          | 18.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 731        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 47         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01151645 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.0328     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0614    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.3e+03     |\n",
      "|    ep_rew_mean          | 17.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 716         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015045138 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 0.000739    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.3e+03     |\n",
      "|    ep_rew_mean          | 17.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 706         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005749948 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0309      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5e+03     |\n",
      "|    ep_rew_mean          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 696         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020060038 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 0.00772     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5e+03     |\n",
      "|    ep_rew_mean          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 687         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008825924 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.00608     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00774    |\n",
      "|    value_loss           | 5.55        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5e+03     |\n",
      "|    ep_rew_mean          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 677         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011314764 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00879    |\n",
      "|    value_loss           | 0.00963     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.44e+03    |\n",
      "|    ep_rew_mean          | 20.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 671         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018904896 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.522       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00793    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.00219     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.44e+03    |\n",
      "|    ep_rew_mean          | 20.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 665         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020418782 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.051      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.33e+03    |\n",
      "|    ep_rew_mean          | 18.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016224418 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00553    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 0.0082      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.33e+03    |\n",
      "|    ep_rew_mean          | 18.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 639         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588918 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.838       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.96e+03    |\n",
      "|    ep_rew_mean          | 21.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011478394 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0246     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    value_loss           | 0.00516     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.96e+03    |\n",
      "|    ep_rew_mean          | 21.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002257682 |\n",
      "|    clip_fraction        | 0.00933     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0832     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.000342   |\n",
      "|    value_loss           | 6.45        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.86e+03    |\n",
      "|    ep_rew_mean          | 20          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014647212 |\n",
      "|    clip_fraction        | 0.0953      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.00028     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00947     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.000288    |\n",
      "|    value_loss           | 0.000463    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.86e+03    |\n",
      "|    ep_rew_mean          | 20          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012307327 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0752      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 1.88        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75e+03    |\n",
      "|    ep_rew_mean          | 19.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017363485 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0222     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.00077     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75e+03    |\n",
      "|    ep_rew_mean          | 19.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 598         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019442298 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.0237      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00713    |\n",
      "|    value_loss           | 1.87        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75e+03    |\n",
      "|    ep_rew_mean          | 19.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 597         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012052266 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00976    |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.67e+03    |\n",
      "|    ep_rew_mean          | 18.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 117         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014246316 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.881       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00993    |\n",
      "|    value_loss           | 0.000166    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.67e+03    |\n",
      "|    ep_rew_mean          | 18.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011195796 |\n",
      "|    clip_fraction        | 0.0979      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 1.02        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.67e+03    |\n",
      "|    ep_rew_mean          | 18.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008246756 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00861    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 0.000462    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.67e+03    |\n",
      "|    ep_rew_mean          | 17.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020719314 |\n",
      "|    clip_fraction        | 0.43        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 0.000346    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.67e+03     |\n",
      "|    ep_rew_mean          | 17.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 587          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0124356635 |\n",
      "|    clip_fraction        | 0.191        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.0752      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0186      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 0.198        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.67e+03   |\n",
      "|    ep_rew_mean          | 17.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 586        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 136        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01468458 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0334     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    value_loss           | 0.00268    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.71e+03    |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 584         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020679556 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0315     |\n",
      "|    value_loss           | 0.00295     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.71e+03    |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 583         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015267112 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.71e+03    |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 582         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009280893 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.786       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00717    |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.73e+03    |\n",
      "|    ep_rew_mean          | 15.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 582         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013734571 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0213      |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 0.0151      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.73e+03    |\n",
      "|    ep_rew_mean          | 15.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 582         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012063071 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00877    |\n",
      "|    value_loss           | 0.172       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.73e+03    |\n",
      "|    ep_rew_mean          | 15.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 582         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019049382 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0098     |\n",
      "|    value_loss           | 0.005       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.76e+03    |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 581         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010905282 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 9.82e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.029       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -3.36e-05   |\n",
      "|    value_loss           | 0.00389     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.76e+03   |\n",
      "|    ep_rew_mean          | 15.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 580        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 165        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01368214 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0206    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.76e+03    |\n",
      "|    ep_rew_mean          | 15.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 580         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 169         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013578444 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.476       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 0.000754    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.78e+03     |\n",
      "|    ep_rew_mean          | 14.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 580          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 172          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067092762 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | -0.00847     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0155       |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00445     |\n",
      "|    value_loss           | 0.000552     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x28f8f4e9c50>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GameEnv()\n",
    "\n",
    "env.reset()\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = os.path.join('Training', 'Models', 'test_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(baseline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "mean_r, mean_d = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12.010203404847072\n",
      "18.135100954954762\n"
     ]
    }
   ],
   "source": [
    "print(mean_r)\n",
    "print(mean_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise(x):\n",
    "    k = 0.0957\n",
    "    y = 0.0092 * np.exp(-k * (x - 10)) - 0.0002\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
