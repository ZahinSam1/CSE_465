{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_name, agent_index):\n",
    "        self.index = agent_index\n",
    "        self.agent = agent_name\n",
    "        self.health = None\n",
    "        self.isHit = False\n",
    "        self.move = True\n",
    "        self.movement_speed = 1.00\n",
    "        self.previous_position = np.array([0, 0], dtype=np.float32)\n",
    "        self.current_position = None\n",
    "        self.same_position = False\n",
    "        self.current_step = 0\n",
    "        self.action = None\n",
    "        pass\n",
    "\n",
    "    def agent_action(self, action):\n",
    "\n",
    "        pass\n",
    "\n",
    "    def agent_reset(self, width, height):\n",
    "        padding = 30\n",
    "        self.current_position = np.array(\n",
    "            [np.random.uniform(30, width - padding), np.random.uniform(30, height - padding)], dtype=np.float32)\n",
    "\n",
    "    def step_update(self, action, range_x, range_y):\n",
    "\n",
    "        if action == 0:\n",
    "            self.current_position[0] -= self.movement_speed\n",
    "        elif action == 1:\n",
    "            self.current_position[0] += self.movement_speed\n",
    "        elif action == 2:\n",
    "            self.current_position[1] -= self.movement_speed\n",
    "        elif action == 3:\n",
    "            self.current_position[1] += self.movement_speed\n",
    "        \n",
    "        self.current_position[0] = np.clip(self.current_position[0], 0, range_x)\n",
    "        self.current_position[1] = np.clip(self.current_position[1], 0, range_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, Box, MultiDiscrete\n",
    "from gymnasium import Env\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "time.sleep(1)\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv(Env):\n",
    "    def __init__(self, screen_width=400, screen_height=400, render_mode='human'):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # defining the observation and action spaces for all the agents\n",
    "        \n",
    "        self.observation_space = Box(low=np.array([0, 0, 0, 0], dtype=np.float32),\n",
    "                                    high=np.array([self.screen_width, self.screen_height, self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "        # the pygame window should be initialized in the render function\n",
    "\n",
    "        # setting the total number of agent\n",
    "        \n",
    "        self.number_of_prey = 1\n",
    "        self.number_of_predator = 1\n",
    "        self.prey_agent = None\n",
    "        self.predator_agent = None\n",
    "        self.predator_i_position = None\n",
    "        self.initial_distance = None\n",
    "        self.current_distance = None\n",
    "        self.predator_total_reward = 0\n",
    "        self.number_of_agents = self.number_of_prey + self.number_of_prey\n",
    "\n",
    "        # defining the action space based on total number of predator and prey\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "\n",
    "        # setting the total number of obstacles\n",
    "        self.total_obstacles = None\n",
    "\n",
    "        # keeping a counter to save the total steps\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # initializing the pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # setting the screen size\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Multi Agent Environment(simple)')\n",
    "        \n",
    "        # keep the track of time of the rendering\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "        # *it is set in milisec format (time is sec is time/1000)\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        # start the tick timer\n",
    "        self.start_time = 0\n",
    "        # print(f'start  time: {self.start_time}')\n",
    "\n",
    "        # initializing the font\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 18)\n",
    "\n",
    "    # this function rerturns the value of the action into 2 digits \n",
    "    # if the action_space.sample() gives 1 digit number\n",
    "    # * if  the number is 3 it will return 03 \n",
    "    # * if  the number is 14 then it will return 14\n",
    "    def expand_action_digit(self, action):\n",
    "\n",
    "        # this basically checks the number if it has 1 then fills the rest with 0\n",
    "        # if the number is 2 digits then it stays the same\n",
    "        action = str(action).zfill(2)\n",
    "        prey_action = int(action[0]) % 4\n",
    "        predator_action = int(action[1]) % 4\n",
    "        return prey_action, predator_action\n",
    "        \n",
    "\n",
    "    # this method will initialize the number of agents\n",
    "    # ! this must be called from outside\n",
    "    def agent_init(self):\n",
    "\n",
    "        prey_agents = Agent('prey', 0)\n",
    "\n",
    "        predator_agents = Agent('predator', 0)\n",
    "\n",
    "        self.prey_agent = prey_agents\n",
    "        self.predator_agent = predator_agents\n",
    "        \n",
    "\n",
    "    # this function is used to explicitly set the number of agents\n",
    "    # ! this needs to be called from outside\n",
    "    def set_agent_number(self, prey_number, predator_number):\n",
    "        self.number_of_predator = predator_number\n",
    "        self.number_of_prey = prey_number\n",
    "\n",
    "    # the usual reset function\n",
    "    def reset(self, seed=0):\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        self.agent_init()\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        prey = self.prey_agent\n",
    "        predator = self.predator_agent\n",
    "\n",
    "        # for prey in self.prey_agents:\n",
    "        prey.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "        # observation.append([prey.index, prey.agent, prey.current_position])\n",
    "\n",
    "        # for predator in self.predator_agents:\n",
    "        predator.agent_reset(width=self.screen_width, height=self.screen_height)\n",
    "        # observation.append([predator.index, predator.agent, predator.current_position])\n",
    "        \n",
    "        # setting the predator and prey to their initial position\n",
    "        self.prey_agent = prey\n",
    "        self.predator_agent = predator\n",
    "\n",
    "        # setting the initial position of predator for reward\n",
    "        self.predator_i_position = self.predator_agent.current_position\n",
    "        \n",
    "        # calculating the initial distance of 2 agents\n",
    "        direction = self.predator_agent.current_position - self.prey_agent.current_position\n",
    "        self.initial_distance = np.linalg.norm(direction)\n",
    "\n",
    "        # observation :\n",
    "        # all the variable values inside the obsercation space needs to be sent inside the observation variable\n",
    "        observation = np.concatenate([self.prey_agent.current_position, self.predator_agent.current_position])\n",
    "        return observation, seed\n",
    "\n",
    "    # the step function\n",
    "    # this function is called for every timesteps\n",
    "    # this function updates the actions or states of agents in the env\n",
    "    # this function is called default by the algorithms of all sorts\n",
    "    # * it returns observation, reward, done, truncated, info\n",
    "    # * any game policy change can be done here\n",
    "    # * reward must be set here\n",
    "    def step(self, action):\n",
    "        \n",
    "        # initializing the return variables\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "        # print(f'current time: {current_time}')\n",
    "        # when ever the step is starting set the start time\n",
    "        # if self.total_steps == 0:\n",
    "        #     self.start_time = pygame.time.get_ticks()\n",
    "        # else:\n",
    "        #     current_time = pygame.time.get_ticks()\n",
    "\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        # print(f'elapsed time: {elapsed_time}')\n",
    "        # handles the pygame window event when closing\n",
    "        # !if the window still crashes pygame.event needs to be managed properly\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                done = True\n",
    "                pygame.quit()\n",
    "        \n",
    "        action =(\"agent1\", \"agent2\")\n",
    "        prey_action, predator_action = action\n",
    "        # print(f'action: {action}')\n",
    "        # *the actions are split as required\n",
    "        # prey_action, predator_action = self.expand_action_digit(action)\n",
    "        # predator_action = action\n",
    "\n",
    "        self.prey_agent.movement_speed = 2\n",
    "        prey = self.prey_agent\n",
    "        prey.step_update(action=prey_action, range_x=self.screen_width - 10, range_y=self.screen_height - 10)\n",
    "        \n",
    "        predator = self.predator_agent\n",
    "        # print(f'prey: {prey_action}, predator: {predator_action}')\n",
    "                \n",
    "\n",
    "        # print(f'predator_{predator.index} = action:{action} current_position: {predator.current_position}')\n",
    "        predator.step_update(action=predator_action, range_x=self.screen_width - 10, range_y=self.screen_height - 10)\n",
    "        # print(f'predator_{predator.index}: new_position: {predator.current_position}')\n",
    "            \n",
    "        # !observation.append({'index': predator.index, 'name': predator.agent, 'position': predator.current_position})\n",
    "        # observation = self.predator_agent.current_position\n",
    "        observation = np.concatenate([self.prey_agent.current_position, self.predator_agent.current_position])\n",
    "\n",
    "        # print(f'observation: {self.predator_agent.current_position}')\n",
    "        self.total_steps += 1\n",
    "\n",
    "        direction = self.predator_agent.current_position - self.prey_agent.current_position\n",
    "\n",
    "        # Calculate the distance between the centers of the two dots\n",
    "        distance_between_centers = np.linalg.norm(direction)\n",
    "        self.current_distance = distance_between_centers\n",
    "\n",
    "        # check everystep if the distance of two agents are:\n",
    "        # greater than  initial distance?  \n",
    "        # yes: -reward\n",
    "        # no: +reward\n",
    "\n",
    "        # if distance_between_centers > self.initial_distance:\n",
    "        #     reward -= 0.06\n",
    "        # else:\n",
    "        #     reward += 0.01\n",
    "\n",
    "        reward = 2 * np.exp(-0.04 * (distance_between_centers - 10)) - 0.001\n",
    "        \n",
    "        # if distance_between_centers > self.initial_distance:\n",
    "        #     reward -= 2 * np.exp(-distance_between_centers * 0.5) - 0.5\n",
    "        # else:\n",
    "        #     reward += 2 * np.exp(-distance_between_centers * 0.5) - 0.5\n",
    "            \n",
    "\n",
    "        # Check if there is a collision (distance <= sum of radii)\n",
    "        if elapsed_time <= self.total_running_time:\n",
    "            if distance_between_centers <= 20:\n",
    "                reward += 40\n",
    "                done = True\n",
    "                    # pygame.quit()\n",
    "                    # self.close()\n",
    "            # if self.total_steps == 30000:\n",
    "            #     done = True\n",
    "        else:\n",
    "            done = True\n",
    "            reward -= 20\n",
    "                # pygame.quit()\n",
    "                # self.close()\n",
    "\n",
    "        # print(self.total_steps)\n",
    "        self.render()\n",
    "        # it will update the total reward everystep\n",
    "        self.predator_total_reward = reward\n",
    "\n",
    "        return observation, reward, done, _, info\n",
    "        \n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            screen = self.screen\n",
    "\n",
    "            # clear screen\n",
    "            screen.fill((255, 255, 255))\n",
    "            prey = self.prey_agent\n",
    "            pos_x, pos_y = prey.current_position\n",
    "            prey_radius = 10\n",
    "            pygame.draw.circle(screen, (0, 0, 255), (int(pos_x), int(pos_y)), prey_radius)\n",
    "\n",
    "            predator = self.predator_agent\n",
    "            pos_x, pos_y = predator.current_position\n",
    "            predator_radius = 10\n",
    "\n",
    "            pygame.draw.circle(screen, (255, 0, 0), (int(pos_x), int(pos_y)), predator_radius)\n",
    "\n",
    "            text_surface = self.font.render(f\"Reward: {self.predator_total_reward: .5f} initial distance: {self.initial_distance: .2f} current_distance:{self.current_distance: .2f}\", True, (0, 0, 0))\n",
    "\n",
    "            text_rect = text_surface.get_rect()\n",
    "\n",
    "            text_rect.center = (self.screen_width - 200, 10)\n",
    "\n",
    "            self.screen.blit(text_surface, text_rect)\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "env.reset()\n",
    "env.step(1) #list value \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(baseline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "environment testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "for i in range(0, 5):\n",
    "    done = False\n",
    "\n",
    "    # env.agent_init()\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    print(f'total reward: {total_reward}')\n",
    "    print(f'Number of steps: {env.total_steps}')\n",
    "        # env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different state entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "model.set_env(env)\n",
    "for i in range(0, 20):\n",
    "    done = False\n",
    "\n",
    "    # env.agent_init()\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        predator_action = model.predict(obs)\n",
    "        prey_action = env.action_space.sample()\n",
    "        action = [prey_action, predator_action[0]]\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    print(f'total reward: {total_reward}')\n",
    "    print(f'Number of steps: {env.total_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.callbacks import EvalCallback\n",
    "import os\n",
    "\n",
    "log_dir = \".\\Training\\Logs\"  # Change this to your desired directory\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# # Create a callback to log data to TensorBoard\n",
    "# callback = EvalCallback(\n",
    "#     eval_env=env,\n",
    "#     callback_on_new_best=log_dir,\n",
    "#     n_eval_episodes=10,  # Adjust as needed\n",
    "#     best_model_save_path=log_dir,\n",
    "#     log_path=log_dir,\n",
    "# )\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()\n",
    "\n",
    "env.reset()\n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = os.path.join('Training', 'Models', 'new_rew_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(baseline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(baseline_path)\n",
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = GameEnv()\n",
    "evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_r)\n",
    "print(mean_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise(x):\n",
    "    k = 0.0957\n",
    "    y = 0.0092 * np.exp(-k * (x - 10)) - 0.0002\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
