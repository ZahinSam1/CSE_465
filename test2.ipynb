{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from Agents.agent import Agent\n",
    "from Agents.RayCast import get_fov_rays\n",
    "from Constants.constants import WHITE, RED, BLUE, SCREEN_WIDTH, SCREEN_HEIGHT, WALLS, WALLS2\n",
    "from Walls.collision_detection import detect_collision\n",
    "from Walls.wall_class import Walls\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GameEnv(Env):\n",
    "    def __init__(self, render_mode='human'):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = SCREEN_WIDTH\n",
    "        self.screen_height = SCREEN_HEIGHT\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        total_values = 219\n",
    "        self.observation_space = Box(\n",
    "            low=np.zeros(total_values, dtype=np.float32),\n",
    "            high=self.screen_width * np.ones(total_values, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Define the radius for the semi-circular reward area\n",
    "        self.semi_circle_radius = 250  # You can adjust this radius\n",
    "\n",
    "        #! Define the center of the screen for the green dot\n",
    "        self.center_x = self.screen_width // 2\n",
    "        self.center_y = self.screen_height // 2\n",
    "\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.number_of_predator = 1\n",
    "\n",
    "        self.predator_agent = None\n",
    "\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        self.obs = None\n",
    "\n",
    "        # start the tick timer\n",
    "        self.start_time = 0\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Multi Agent Environment(simple)')\n",
    "\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 18)\n",
    "\n",
    "        self.wall = Walls(pygame)\n",
    "        self.walls = None\n",
    "\n",
    "    def agent_init(self):\n",
    "        predator_agents = Agent('predator', 0)\n",
    "        self.predator_agent = predator_agents\n",
    "\n",
    "    def flatten_list(self, nested_list):\n",
    "        flattened_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flattened_list.extend(self.flatten_list(item))\n",
    "            else:\n",
    "                flattened_list.append(item)\n",
    "        return flattened_list\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        observation = []\n",
    "        agent_pos = [self.predator_agent.current_position[0], self.predator_agent.current_position[1]]\n",
    "        observation.append(agent_pos)\n",
    "\n",
    "        angle = self.predator_agent.angle\n",
    "        observation.append(angle)\n",
    "\n",
    "        value_list = get_fov_rays(agent_pos)\n",
    "        observation.append(value_list)\n",
    "        \n",
    "        observation = self.flatten_list(observation)\n",
    "        return observation\n",
    "\n",
    "    def _max_right(self):\n",
    "        max_right = 0\n",
    "        for wall in self.walls:\n",
    "            if wall.right > max_right:\n",
    "                max_right = wall.right\n",
    "        return max_right\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        self.start_time = time.time()\n",
    "        self.agent_init()\n",
    "        self.wall.clear_walls()\n",
    "        self.walls = self.wall.make_wall(WALLS2)\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        predator = self.predator_agent\n",
    "        predator.agent_reset(width=self.screen_width, height=self.screen_height, walls=self.walls)\n",
    "\n",
    "        self.predator_agent = predator\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        self.obs = observation\n",
    "\n",
    "        return observation, seed\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                done = True\n",
    "                pygame.quit()\n",
    "        self.predator_agent.step_update(action, range_x=self.screen_width, range_y=self.screen_height)\n",
    "        self.predator_agent = detect_collision(self.predator_agent, self.walls)\n",
    "\n",
    "        # Calculate the center of the exponential reward area\n",
    "        center_x, center_y = 325, 300\n",
    "        # Calculate the distance from the agent to the center\n",
    "        distance_to_center = ((self.predator_agent.current_position[0] - center_x) ** 2 + (self.predator_agent.current_position[1] - center_y) ** 2) ** 0.5\n",
    "        # Set the radius for the exponential reward function to start working\n",
    "        radius_for_exponential_reward = 100  # Adjust this radius as needed\n",
    "        # Check if the agent is within the exponential reward area\n",
    "        if distance_to_center <= radius_for_exponential_reward:\n",
    "            reward = 10 * np.exp(-1 * distance_to_center / 100) + 0.05\n",
    "\n",
    "        if self.predator_agent.current_position[0] > self._max_right():\n",
    "            reward += 100\n",
    "            done = True\n",
    "\n",
    "        if elapsed_time >= self.total_running_time:\n",
    "            reward -= 50\n",
    "            done = True\n",
    "\n",
    "        reward += 0.01\n",
    "        self.render()\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        self.predator_total_reward = reward\n",
    "        self.obs = observation\n",
    "\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            screen = self.screen\n",
    "            screen.fill(WHITE)\n",
    "            \n",
    "            predator = self.predator_agent\n",
    "            pygame.draw.circle(screen, RED, predator.center, predator.radius)\n",
    "            pygame.draw.line(screen, RED, predator.center, predator.draw_direction_end, 5)\n",
    "\n",
    "            for key, wall in WALLS2.items():\n",
    "                pygame.draw.rect(screen, BLUE, (wall['x'], wall['y'], wall['width'], wall['height']))\n",
    "\n",
    "            max_right = self._max_right()\n",
    "            pygame.draw.circle(screen, (0, 255, 0), (325, 300), 5)\n",
    "\n",
    "            max_right_color = (255, 0, 0)\n",
    "            pygame.draw.line(self.screen, max_right_color, (max_right, 0), (max_right, self.screen_height), 2)\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs', 'Level_01_PPO')\n",
    "baseline_path = os.path.join('Training', 'Models', 'Level_01_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training\\Logs\\Level_01_PPO\\PPO_12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 667      |\n",
      "|    ep_rew_mean     | -43.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 66       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 629         |\n",
      "|    ep_rew_mean          | -43.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010956144 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.00521     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.1        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 30.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 613         |\n",
      "|    ep_rew_mean          | -43.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010294493 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0207      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.77        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 616         |\n",
      "|    ep_rew_mean          | -43.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009849209 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.00234    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 620         |\n",
      "|    ep_rew_mean          | 62.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 165         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012362569 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.0125      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 27.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005882982 |\n",
      "|    clip_fraction        | 0.00649     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.000287    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 628         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.000583   |\n",
      "|    value_loss           | 997         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | 154         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013916569 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.0027      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00511    |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 620           |\n",
      "|    ep_rew_mean          | 131           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 61            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 264           |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023353423 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.59         |\n",
      "|    explained_variance   | 0.00748       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 771           |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000171     |\n",
      "|    value_loss           | 1.57e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 620         |\n",
      "|    ep_rew_mean          | 113         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 297         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020477906 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.0103     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    value_loss           | 45.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 620         |\n",
      "|    ep_rew_mean          | 94.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 331         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011098586 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00141    |\n",
      "|    value_loss           | 41.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 621          |\n",
      "|    ep_rew_mean          | 82.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 363          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062079257 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.58        |\n",
      "|    explained_variance   | -4.45e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.6         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00351     |\n",
      "|    value_loss           | 50.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | 73          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 396         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016982503 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.00803    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.5        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | 64.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 429         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013939651 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 33.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | 55.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 462         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014028633 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.3        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00535    |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 621          |\n",
      "|    ep_rew_mean          | 49.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 495          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029761633 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.6         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000792    |\n",
      "|    value_loss           | 34.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 621         |\n",
      "|    ep_rew_mean          | 43.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 528         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016665831 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.00095     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    value_loss           | 29.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 620         |\n",
      "|    ep_rew_mean          | 37.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010386366 |\n",
      "|    clip_fraction        | 0.0445      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 28.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 619         |\n",
      "|    ep_rew_mean          | 92.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011494217 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.32        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 36.7        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 618           |\n",
      "|    ep_rew_mean          | 85.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 61            |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 630           |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051354483 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+03      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | 0.000279      |\n",
      "|    value_loss           | 2.13e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 619          |\n",
      "|    ep_rew_mean          | 78           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 663          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044797263 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.4         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000298    |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 618         |\n",
      "|    ep_rew_mean          | 72.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 697         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014444326 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.9        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 48.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 617        |\n",
      "|    ep_rew_mean          | 67.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 730        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01249175 |\n",
      "|    clip_fraction        | 0.0563     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.51      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 40.4       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00143   |\n",
      "|    value_loss           | 42         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 619         |\n",
      "|    ep_rew_mean          | 62          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 763         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016748138 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.26        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 38.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 619         |\n",
      "|    ep_rew_mean          | 58          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 796         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011400134 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00636    |\n",
      "|    value_loss           | 37.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 618         |\n",
      "|    ep_rew_mean          | 54.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 830         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007263739 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    value_loss           | 33.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2378db54a90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "env.reset()\n",
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(baseline_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from Agents.agent import Agent\n",
    "from Agents.RayCast import get_fov_rays\n",
    "from Constants.constants import WHITE, RED, BLUE, SCREEN_WIDTH, SCREEN_HEIGHT, WALLS, WALLS2\n",
    "from Walls.collision_detection import detect_collision\n",
    "from Walls.wall_class import Walls\n",
    "\n",
    "class GameEnv(Env):\n",
    "    def __init__(self, render_mode='human'):\n",
    "        super(GameEnv, self).__init__()\n",
    "\n",
    "        # defining the screen dimension for render purpose\n",
    "        self.screen_width = SCREEN_WIDTH\n",
    "        self.screen_height = SCREEN_HEIGHT\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        total_values = 219\n",
    "        self.observation_space = Box(\n",
    "            low=np.zeros(total_values, dtype=np.float32),\n",
    "            high=self.screen_width * np.ones(total_values, dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Define the radius for the semi-circular reward area\n",
    "        self.semi_circle_radius = 250  # You can adjust this radius\n",
    "\n",
    "        #! Define the center of the screen for the green dot\n",
    "        self.center_x = self.screen_width // 2\n",
    "        self.center_y = self.screen_height // 2\n",
    "\n",
    "        self.action_space = Discrete(5)\n",
    "\n",
    "        self.total_steps = 0\n",
    "\n",
    "        self.number_of_predator = 1\n",
    "\n",
    "        self.predator_agent = None\n",
    "\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        self.obs = None\n",
    "\n",
    "        # start the tick timer\n",
    "        self.start_time = 0\n",
    "        self.total_running_time = 10\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Multi Agent Environment(simple)')\n",
    "\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 18)\n",
    "\n",
    "        self.wall = Walls(pygame)\n",
    "        self.walls = None\n",
    "\n",
    "    def agent_init(self):\n",
    "        predator_agents = Agent('predator', 0)\n",
    "        self.predator_agent = predator_agents\n",
    "\n",
    "    def flatten_list(self, nested_list):\n",
    "        flattened_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flattened_list.extend(self.flatten_list(item))\n",
    "            else:\n",
    "                flattened_list.append(item)\n",
    "        return flattened_list\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        observation = []\n",
    "        agent_pos = [self.predator_agent.current_position[0], self.predator_agent.current_position[1]]\n",
    "        observation.append(agent_pos)\n",
    "\n",
    "        angle = self.predator_agent.angle\n",
    "        observation.append(angle)\n",
    "\n",
    "        value_list = get_fov_rays(agent_pos)\n",
    "        observation.append(value_list)\n",
    "        \n",
    "        observation = self.flatten_list(observation)\n",
    "        return observation\n",
    "\n",
    "    def _max_right(self):\n",
    "        max_right = 0\n",
    "        for wall in self.walls:\n",
    "            if wall.right > max_right:\n",
    "                max_right = wall.right\n",
    "        return max_right\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        self.start_time = time.time()\n",
    "        self.agent_init()\n",
    "        self.wall.clear_walls()\n",
    "        self.walls = self.wall.make_wall(WALLS2)\n",
    "        self.total_steps = 0\n",
    "        self.predator_total_reward = 0\n",
    "\n",
    "        predator = self.predator_agent\n",
    "        predator.agent_reset(width=self.screen_width, height=self.screen_height, walls=self.walls)\n",
    "\n",
    "        self.predator_agent = predator\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        self.obs = observation\n",
    "\n",
    "        return observation, seed\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        current_time = time.time()\n",
    "\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                done = True\n",
    "                pygame.quit()\n",
    "        self.predator_agent.step_update(action, range_x=self.screen_width, range_y=self.screen_height)\n",
    "        self.predator_agent = detect_collision(self.predator_agent, self.walls)\n",
    "\n",
    "        self.total_steps += 1\n",
    "        # Calculate the distance to the center (325, 300)\n",
    "        center_x = 325\n",
    "        center_y = 300\n",
    "        distance_to_center = ((self.predator_agent.current_position[0] - center_x) ** 2 + (self.predator_agent.current_position[1] - center_y) ** 2) ** 0.5\n",
    "        # Use the given exponential reward function\n",
    "        reward = 100 * np.exp(-2 * distance_to_center / 100) - 1.5\n",
    "         \n",
    "        if self.predator_agent.current_position[0] > self._max_right():\n",
    "            reward += 100\n",
    "            done = True\n",
    "\n",
    "        if elapsed_time >= self.total_running_time:\n",
    "            reward -= 50\n",
    "            done = True\n",
    "      \n",
    "        reward += 0.01\n",
    "        self.render()\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        self.predator_total_reward = reward\n",
    "        self.obs = observation\n",
    "\n",
    "        return observation, reward, done, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            screen = self.screen\n",
    "            screen.fill(WHITE)\n",
    "            \n",
    "            predator = self.predator_agent\n",
    "            pygame.draw.circle(screen, RED, predator.center, predator.radius)\n",
    "            pygame.draw.line(screen, RED, predator.center, predator.draw_direction_end, 5)\n",
    "\n",
    "            for key, wall in WALLS2.items():\n",
    "                pygame.draw.rect(screen, BLUE, (wall['x'], wall['y'], wall['width'], wall['height']))\n",
    "\n",
    "\n",
    "            # Calculate the center of the exponential reward area\n",
    "            center_x, center_y = 325, 300\n",
    "            # Calculate the distance from the agent to the center\n",
    "            distance_to_center = ((self.predator_agent.current_position[0] - center_x) ** 2 + (self.predator_agent.current_position[1] - center_y) ** 2) ** 0.5\n",
    "            # Check if the agent is within the exponential reward area\n",
    "            if distance_to_center <= 250:  # Adjust this radius as needed\n",
    "                reward = 100 * np.exp(-2 * distance_to_center / 100) - 1.5\n",
    "\n",
    "\n",
    "            max_right = self._max_right()\n",
    "            pygame.draw.circle(screen, (0, 255, 0), (325, 300), 5)\n",
    "\n",
    "            max_right_color = (255, 0, 0)\n",
    "            pygame.draw.line(self.screen, max_right_color, (max_right, 0), (max_right, self.screen_height), 2)\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "\n",
    "def main():\n",
    "    # Initialize the environment\n",
    "    env = GameEnv(render_mode='human')\n",
    "\n",
    "    # Reset the environment to get the initial observation\n",
    "    observation, seed = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Replace this with your agent's logic to choose actions\n",
    "        # In this example, we're taking a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Step through the environment with the chosen action\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Close the environment when done\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set up display\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Predator Agent Environment\")\n",
    "\n",
    "# Define colors\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "# Predator agent position and size\n",
    "predator_x, predator_y = WIDTH // 2, HEIGHT // 2\n",
    "predator_radius = 10\n",
    "move_step = 1\n",
    "\n",
    "# Define objects with arbitrary shapes\n",
    "objects = {\n",
    "    \"special_wall\": [(100, 100), (120, 100), (120, 200), (100, 200)],\n",
    "    \"wall2\": [(200, 300), (220, 300), (220, 400), (200, 400)],\n",
    "    \"wall3\": [(300, 200), (320, 200), (320, 300), (300, 300)],\n",
    "    \"circular_wall\": [(400, 400), (430, 400), (430, 430), (400, 430)]\n",
    "}\n",
    "\n",
    "def is_ray_blocked(ray_start, ray_angle):\n",
    "    closest_intersection = None\n",
    "\n",
    "    x1, y1 = ray_start\n",
    "    x2, y2 = x1 + 1000 * math.cos(math.radians(ray_angle)), y1 + 1000 * math.sin(math.radians(ray_angle))\n",
    "\n",
    "    for object_name, object_vertices in objects.items():\n",
    "        for i in range(len(object_vertices)):\n",
    "            x3, y3 = object_vertices[i]\n",
    "            x4, y4 = object_vertices[(i + 1) % len(object_vertices)]\n",
    "\n",
    "            denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n",
    "\n",
    "            if denominator == 0:\n",
    "                continue\n",
    "\n",
    "            t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denominator\n",
    "            u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / denominator\n",
    "\n",
    "            epsilon = 1e-5  # Small epsilon value\n",
    "\n",
    "            if -epsilon <= t <= 1 + epsilon and 0 <= u <= 1:\n",
    "                intersection_x = x1 + t * (x2 - x1)\n",
    "                intersection_y = y1 + t * (y2 - y1)\n",
    "\n",
    "                distance = math.hypot(intersection_x - x1, intersection_y - y1)\n",
    "                if closest_intersection is None or distance < closest_intersection:\n",
    "                    closest_intersection = distance\n",
    "\n",
    "    return closest_intersection\n",
    "\n",
    "# Main game loop\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        predator_x -= move_step\n",
    "    if keys[pygame.K_RIGHT]:\n",
    "        predator_x += move_step\n",
    "    if keys[pygame.K_UP]:\n",
    "        predator_y -= move_step\n",
    "    if keys[pygame.K_DOWN]:\n",
    "        predator_y += move_step\n",
    "\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    for angle in range(360):\n",
    "        ray_start = (predator_x, predator_y)\n",
    "        ray_angle = angle\n",
    "        closest_intersection = is_ray_blocked(ray_start, ray_angle)\n",
    "\n",
    "        if closest_intersection is not None:\n",
    "            ray_end_x = ray_start[0] + closest_intersection * math.cos(math.radians(ray_angle))\n",
    "            ray_end_y = ray_start[1] + closest_intersection * math.sin(math.radians(ray_angle))\n",
    "            pygame.draw.line(screen, RED, ray_start, (ray_end_x, ray_end_y), 1)\n",
    "        else:\n",
    "            ray_end_x = ray_start[0] + 1000 * math.cos(math.radians(ray_angle))\n",
    "            ray_end_y = ray_start[1] + 1000 * math.sin(math.radians(ray_angle))\n",
    "            pygame.draw.line(screen, BLUE, ray_start, (ray_end_x, ray_end_y), 1)\n",
    "\n",
    "    pygame.draw.circle(screen, RED, (predator_x, predator_y), predator_radius)\n",
    "\n",
    "    for object_name, object_vertices in objects.items():\n",
    "        pygame.draw.polygon(screen, GREEN, object_vertices)\n",
    "        \n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set up display\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Predator Agent Environment\")\n",
    "\n",
    "# Define colors\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)  # Define green color\n",
    "BLUE = (0, 0, 255)   # Define blue color for vision rays\n",
    "\n",
    "# Predator agent position and size\n",
    "predator_x, predator_y = WIDTH // 2, HEIGHT // 2\n",
    "predator_radius = 10  # Radius of the circular predator agent\n",
    "move_step = 1  # Adjust the movement step\n",
    "\n",
    "# Define walls in a dictionary\n",
    "walls = {\n",
    "    \"special_wall\": {\"x\": 100, \"y\": 100, \"width\": 20, \"height\": 100},\n",
    "    \"wall2\": {\"x\": 200, \"y\": 300, \"width\": 20, \"height\": 100},\n",
    "    \"wall3\": {\"x\": 300, \"y\": 200, \"width\": 20, \"height\": 100},\n",
    "}\n",
    "\n",
    "# Function to check ray-wall intersection\n",
    "def is_ray_blocked(ray_start, ray_angle):\n",
    "    closest_intersection = None\n",
    "    blocking_wall = None\n",
    "\n",
    "    x1, y1 = ray_start\n",
    "    x2, y2 = x1 + 1000 * math.cos(math.radians(ray_angle)), y1 + 1000 * math.sin(math.radians(ray_angle))\n",
    "\n",
    "    for wall_name, wall in walls.items():\n",
    "        x3, y3 = wall[\"x\"], wall[\"y\"]\n",
    "        x4, y4 = x3 + wall[\"width\"], y3 + wall[\"height\"]\n",
    "\n",
    "        denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n",
    "\n",
    "        if denominator == 0:\n",
    "            continue\n",
    "\n",
    "        t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denominator\n",
    "        u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / denominator\n",
    "\n",
    "        if 0 <= t <= 1 and 0 <= u <= 1:\n",
    "            intersection_x = x1 + t * (x2 - x1)\n",
    "            intersection_y = y1 + t * (y2 - y1)\n",
    "\n",
    "            if not closest_intersection or t < closest_intersection:\n",
    "                closest_intersection = t\n",
    "                blocking_wall = wall_name\n",
    "\n",
    "    return closest_intersection, blocking_wall\n",
    "\n",
    "# Main game loop\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    # Handle user input\n",
    "    keys = pygame.key.get_pressed()\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        predator_x -= move_step\n",
    "    if keys[pygame.K_RIGHT]:\n",
    "        predator_x += move_step\n",
    "    if keys[pygame.K_UP]:\n",
    "        predator_y -= move_step\n",
    "    if keys[pygame.K_DOWN]:\n",
    "        predator_y += move_step\n",
    "\n",
    "    # Clear the screen\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    # Draw vision rays\n",
    "    for angle in range(360):\n",
    "        ray_start = (predator_x, predator_y)\n",
    "        ray_angle = angle\n",
    "        ray_end = (ray_start[0] + 1000 * math.cos(math.radians(ray_angle)),\n",
    "                   ray_start[1] + 1000 * math.sin(math.radians(ray_angle)))\n",
    "\n",
    "        # Check if the ray is blocked by any wall\n",
    "        closest_intersection, blocking_wall = is_ray_blocked(ray_start, ray_angle)\n",
    "\n",
    "        if closest_intersection is not None:\n",
    "            ray_end_x = ray_start[0] + closest_intersection * (ray_end[0] - ray_start[0])\n",
    "            ray_end_y = ray_start[1] + closest_intersection * (ray_end[1] - ray_start[1])\n",
    "\n",
    "            if blocking_wall == \"special_wall\":\n",
    "                color = RED\n",
    "            else:\n",
    "                color = GREEN\n",
    "\n",
    "            pygame.draw.line(screen, color, ray_start, (ray_end_x, ray_end_y), 1)\n",
    "        else:\n",
    "            pygame.draw.line(screen, BLUE, ray_start, ray_end, 1)\n",
    "\n",
    "    # Draw the predator agent as a circle\n",
    "    pygame.draw.circle(screen, RED, (predator_x, predator_y), predator_radius)\n",
    "\n",
    "    # Draw walls from the dictionary with green color\n",
    "    for wall in walls.values():\n",
    "        pygame.draw.rect(screen, GREEN, (wall[\"x\"], wall[\"y\"], wall[\"width\"], wall[\"height\"]))\n",
    "\n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Quit Pygame\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import math\n",
    "\n",
    "class GameEnv:\n",
    "    def __init__(self, width, height):\n",
    "        pygame.init()\n",
    "        self.WIDTH = width\n",
    "        self.HEIGHT = height\n",
    "        self.screen = pygame.display.set_mode((self.WIDTH, self.HEIGHT))\n",
    "        pygame.display.set_caption(\"Predator Agent Environment\")\n",
    "\n",
    "        # Define colors\n",
    "        self.RED = (255, 0, 0)\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.BLUE = (0, 0, 255)\n",
    "\n",
    "        # Predator agent position and size\n",
    "        self.predator_x, self.predator_y = self.WIDTH // 2, self.HEIGHT // 2\n",
    "        self.predator_radius = 10\n",
    "        self.move_step = 1\n",
    "\n",
    "        # Define objects with arbitrary shapes\n",
    "        self.objects = {\n",
    "            \"special_wall\": [(100, 100), (120, 100), (120, 200), (100, 200)],\n",
    "            \"wall2\": [(200, 300), (220, 300), (220, 400), (200, 400)],\n",
    "            \"wall3\": [(300, 200), (320, 200), (320, 300), (300, 300)],\n",
    "            \"circular_wall\": [(400, 400), (430, 400), (430, 430), (400, 430)]\n",
    "        }\n",
    "\n",
    "    def is_ray_blocked(self, ray_start, ray_angle):\n",
    "        closest_intersection = None\n",
    "\n",
    "        x1, y1 = ray_start\n",
    "        x2, y2 = x1 + 1000 * math.cos(math.radians(ray_angle)), y1 + 1000 * math.sin(math.radians(ray_angle))\n",
    "\n",
    "        for object_name, object_vertices in self.objects.items():\n",
    "            for i in range(len(object_vertices)):\n",
    "                x3, y3 = object_vertices[i]\n",
    "                x4, y4 = object_vertices[(i + 1) % len(object_vertices)]\n",
    "\n",
    "                denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n",
    "\n",
    "                if denominator == 0:\n",
    "                    continue\n",
    "\n",
    "                t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denominator\n",
    "                u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / denominator\n",
    "\n",
    "                epsilon = 1e-5  # Small epsilon value\n",
    "\n",
    "                if -epsilon <= t <= 1 + epsilon and 0 <= u <= 1:\n",
    "                    intersection_x = x1 + t * (x2 - x1)\n",
    "                    intersection_y = y1 + t * (y2 - y1)\n",
    "\n",
    "                    distance = math.hypot(intersection_x - x1, intersection_y - y1)\n",
    "                    if closest_intersection is None or distance < closest_intersection:\n",
    "                        closest_intersection = distance\n",
    "\n",
    "        return closest_intersection\n",
    "\n",
    "    def render(self):\n",
    "        self.screen.fill(self.WHITE)\n",
    "\n",
    "        for angle in range(360):\n",
    "            ray_start = (self.predator_x, self.predator_y)\n",
    "            ray_angle = angle\n",
    "            closest_intersection = self.is_ray_blocked(ray_start, ray_angle)\n",
    "\n",
    "            if closest_intersection is not None:\n",
    "                ray_end_x = ray_start[0] + closest_intersection * math.cos(math.radians(ray_angle))\n",
    "                ray_end_y = ray_start[1] + closest_intersection * math.sin(math.radians(ray_angle))\n",
    "                pygame.draw.line(self.screen, self.RED, ray_start, (ray_end_x, ray_end_y), 1)\n",
    "            else:\n",
    "                ray_end_x = ray_start[0] + 1000 * math.cos(math.radians(ray_angle))\n",
    "                ray_end_y = ray_start[1] + 1000 * math.sin(math.radians(ray_angle))\n",
    "                pygame.draw.line(self.screen, self.BLUE, ray_start, (ray_end_x, ray_end_y), 1)\n",
    "\n",
    "        pygame.draw.circle(self.screen, self.RED, (self.predator_x, self.predator_y), self.predator_radius)\n",
    "\n",
    "        for object_name, object_vertices in self.objects.items():\n",
    "            pygame.draw.polygon(self.screen, self.GREEN, object_vertices)\n",
    "\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def run(self):\n",
    "        running = True\n",
    "        while running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    running = False\n",
    "\n",
    "            keys = pygame.key.get_pressed()\n",
    "            if keys[pygame.K_LEFT]:\n",
    "                self.predator_x -= self.move_step\n",
    "            if keys[pygame.K_RIGHT]:\n",
    "                self.predator_x += self.move_step\n",
    "            if keys[pygame.K_UP]:\n",
    "                self.predator_y -= self.move_step\n",
    "            if keys[pygame.K_DOWN]:\n",
    "                self.predator_y += self.move_step\n",
    "\n",
    "            self.render()\n",
    "\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    game = GameEnv(800, 600)\n",
    "    game.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set up display\n",
    "WIDTH, HEIGHT = 800, 600\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Predator Agent Environment\")\n",
    "\n",
    "# Define colors\n",
    "RED = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "# Predator agent position and size\n",
    "predator_x, predator_y = WIDTH // 2, HEIGHT // 2\n",
    "predator_radius = 10\n",
    "move_step = 1\n",
    "\n",
    "# Define objects with arbitrary shapes\n",
    "objects = {\n",
    "    # \"wall1\": [(100, 100), (120, 100), (120, 200), (100, 200)],\n",
    "    \"wall2\": [(200, 300), (220, 300), (220, 400), (200, 400)],\n",
    "    \"wall3\": [(300, 200), (320, 200), (320, 300), (300, 300)],\n",
    "    \"circular_wall\": [(400, 400), (430, 400), (430, 430), (400, 430)],\n",
    "    \"special_point\": (100, 100)  # This is now a single point\n",
    "}\n",
    "\n",
    "# Function to check ray-object intersection\n",
    "def does_agent_see_special_point(ray_start, ray_angle, special_point, objects):\n",
    "    x1, y1 = ray_start\n",
    "    x2, y2 = x1 + 1000 * math.cos(math.radians(ray_angle)), y1 + 1000 * math.sin(math.radians(ray_angle))\n",
    "\n",
    "    for object_name, object in objects.items():\n",
    "        if object_name == \"special_point\":\n",
    "            continue  # Skip the special_point, as it's not a polygon\n",
    "\n",
    "        for i in range(len(object)):\n",
    "            x3, y3 = object[i]\n",
    "            x4, y4 = object[(i + 1) % len(object)]\n",
    "\n",
    "            denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n",
    "\n",
    "            if denominator == 0:\n",
    "                continue\n",
    "\n",
    "            t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denominator\n",
    "            u = -((x1 - x2) * (y1 - y3) - (y1 - y2) * (x1 - x3)) / denominator\n",
    "\n",
    "            if 0 <= t <= 1 and 0 <= u <= 1:\n",
    "                intersection_x = x1 + t * (x2 - x1)\n",
    "                intersection_y = y1 + t * (y2 - y1)\n",
    "\n",
    "                # Check if the intersection point is the special point\n",
    "                if (abs(intersection_x - special_point[0]) < 1 and\n",
    "                        abs(intersection_y - special_point[1]) < 1):\n",
    "                    return True  # Agent sees the special point\n",
    "\n",
    "    return False  # Agent does not see the special point\n",
    "\n",
    "# Main game loop\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "    if keys[pygame.K_LEFT]:\n",
    "        predator_x -= move_step\n",
    "    if keys[pygame.K_RIGHT]:\n",
    "        predator_x += move_step\n",
    "    if keys[pygame.K_UP]:\n",
    "        predator_y -= move_step\n",
    "    if keys[pygame.K_DOWN]:\n",
    "        predator_y += move_step\n",
    "\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    for angle in range(360):\n",
    "        ray_start = (predator_x, predator_y)\n",
    "        ray_angle = angle\n",
    "\n",
    "        sees_special_point = does_agent_see_special_point(ray_start, ray_angle, objects[\"special_point\"], objects)\n",
    "\n",
    "        ray_end = (ray_start[0] + 1000 * math.cos(math.radians(ray_angle)),\n",
    "                   ray_start[1] + 1000 * math.sin(math.radians(ray_angle)))\n",
    "\n",
    "        if sees_special_point:\n",
    "            color = RED\n",
    "        else:\n",
    "            color = BLUE\n",
    "\n",
    "        pygame.draw.line(screen, color, ray_start, ray_end, 1)\n",
    "\n",
    "    pygame.draw.circle(screen, RED, (predator_x, predator_y), predator_radius)\n",
    "\n",
    "    for object_name, object in objects.items():\n",
    "        if object_name == \"special_point\":\n",
    "            continue\n",
    "        pygame.draw.polygon(screen, GREEN, object)\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "pygame.quit()\n",
    "sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
