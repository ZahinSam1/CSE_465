{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import pygame\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotEnv(gym.Env):\n",
    "    def __init__(self, screen_width=700, screen_height=700, render_mode='human'):\n",
    "        super(DotEnv, self).__init__()\n",
    "\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.blue_dot_radius = 30\n",
    "        self.blue_dot_health = 50\n",
    "        self.blue_dot_speed = 4.0  # Increased speed\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        self.food_radius = 20\n",
    "        self.food_positions = []  # List to store food positions\n",
    "        self.food_types = []  # List to store food types ('pink', 'green')\n",
    "\n",
    "        self.eaten_pink_food = 0\n",
    "        self.reward_bonus_interval = 5  # Bonus reward every 5 pink foods\n",
    "\n",
    "        self.time_limit = 10  # Episode time limit in seconds\n",
    "        self.episode_start_time = 0\n",
    "        self.episode_number = 0  # Initialize episode number\n",
    "        self.elapsed_time = 0  # Initialize elapsed time\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=np.array([0, 0], dtype=np.float32),\n",
    "            high=np.array([self.screen_width / 2, self.screen_height], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Dots Moving Environment')\n",
    "\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.max_food_count = 2  # Maximum of 2 food items (1 pink, 1 green)\n",
    "        self.dynamic_reward = 0  # Initialize dynamic reward\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def spawn_food(self):\n",
    "        while True:\n",
    "            # Generate a random position within the circular boundary\n",
    "            food_position = np.array(\n",
    "                [random.uniform(self.blue_dot_radius, self.screen_width / 2 - self.food_radius),\n",
    "                 random.uniform(self.blue_dot_radius, self.screen_height - self.food_radius)],\n",
    "                dtype=np.float32)\n",
    "\n",
    "            # Check if the food position is within the circular boundary\n",
    "            distance_to_center = np.linalg.norm(\n",
    "                food_position - np.array([self.screen_width / 2, self.screen_height / 2]))\n",
    "            if distance_to_center < self.screen_width / 2 - 1:\n",
    "                # The food position is within the circular boundary\n",
    "                break\n",
    "\n",
    "        food_type = 'pink' if len(self.food_positions) % 2 == 0 else 'green'\n",
    "        self.food_positions.append(food_position)\n",
    "        self.food_types.append(food_type)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.episode_start_time = time.time()  # Reset episode start time\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        self.food_positions = []\n",
    "        self.food_types = []\n",
    "        self.total_reward = 0\n",
    "        self.eaten_pink_food = 0\n",
    "        self.episode_number += 1  # Increment episode number\n",
    "        self.elapsed_time = 0  # Reset elapsed time\n",
    "\n",
    "        for _ in range(self.max_food_count):\n",
    "            self.spawn_food()\n",
    "\n",
    "        observation = np.concatenate([self.blue_dot_pos])\n",
    "\n",
    "        return [observation, seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        elapsed_time = time.time() - self.episode_start_time\n",
    "        self.elapsed_time = elapsed_time  # Update elapsed time\n",
    "\n",
    "        move_speed = self.blue_dot_speed\n",
    "\n",
    "        if action == 0:\n",
    "            self.blue_dot_pos[0] -= move_speed\n",
    "        elif action == 1:\n",
    "            self.blue_dot_pos[0] += move_speed\n",
    "        elif action == 2:\n",
    "            self.blue_dot_pos[1] -= move_speed\n",
    "        elif action == 3:\n",
    "            self.blue_dot_pos[1] += move_speed\n",
    "\n",
    "        # Ensure the blue dot stays within the environment boundaries\n",
    "        self.blue_dot_pos[0] = np.clip(self.blue_dot_pos[0], self.blue_dot_radius,\n",
    "                                       self.screen_width / 2 - self.blue_dot_radius)\n",
    "        self.blue_dot_pos[1] = np.clip(self.blue_dot_pos[1], self.blue_dot_radius,\n",
    "                                       self.screen_height - self.blue_dot_radius)\n",
    "\n",
    "        reward = 0\n",
    "        for i in range(len(self.food_positions)):\n",
    "            food_position = self.food_positions[i]\n",
    "            food_type = self.food_types[i]\n",
    "            distance_to_food = np.linalg.norm(self.blue_dot_pos - food_position)\n",
    "\n",
    "            if distance_to_food < self.blue_dot_radius + self.food_radius:\n",
    "                if food_type == 'pink':\n",
    "                    reward += 75  # Increased reward for eating pink\n",
    "                    self.eaten_pink_food += 1\n",
    "                    self.total_reward += reward\n",
    "                    self.dynamic_reward = 75  # Set dynamic reward to 75 after eating pink\n",
    "                    del self.food_positions[i]\n",
    "                    del self.food_types[i]\n",
    "                    self.episode_start_time = time.time()  # Restart episode\n",
    "                    return np.copy([self.blue_dot_pos]), reward, True, {}\n",
    "                elif food_type == 'green':\n",
    "                    reward -= 50  # Penalty for eating green\n",
    "                    self.total_reward += reward\n",
    "                    self.dynamic_reward = 25  # Set dynamic reward to 25 after eating green\n",
    "                    del self.food_positions[i]\n",
    "                    del self.food_types[i]\n",
    "                    break\n",
    "            else:\n",
    "                if distance_to_food < self.blue_dot_radius + self.food_radius * 2:\n",
    "                    if food_type == 'pink':\n",
    "                        reward += 5  # Proximity reward for pink\n",
    "                        self.dynamic_reward = 5  # Set dynamic reward to 5 when close to pink\n",
    "                    elif food_type == 'green':\n",
    "                        reward -= 5  # Proximity penalty for green\n",
    "                        self.dynamic_reward = -5  # Set dynamic reward to -5 when close to green\n",
    "\n",
    "        if elapsed_time >= self.time_limit:\n",
    "            reward -= 50  # Time's up penalty\n",
    "            self.dynamic_reward = -50  # Set dynamic reward to -50 after time's up\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        if self.eaten_pink_food % self.reward_bonus_interval == 0:\n",
    "            reward += 100  # Bonus reward for every 5 pink foods\n",
    "            self.dynamic_reward += 100  # Increase dynamic reward by 100 for bonus\n",
    "\n",
    "        return np.copy([self.blue_dot_pos]), reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            self.screen.fill((93, 97, 140))\n",
    "\n",
    "            pygame.draw.circle(self.screen, (234, 222, 255), (int(self.screen_width / 2), int(self.screen_height / 2)),\n",
    "                               int(self.screen_width / 2) - 1)\n",
    "            pygame.draw.circle(self.screen, (0, 0, 0), (int(self.screen_width / 2), int(self.screen_height / 2)),\n",
    "                               int(self.screen_width / 2), 1)\n",
    "\n",
    "            for i in range(len(self.food_positions)):\n",
    "                food_position = self.food_positions[i]\n",
    "                food_type = self.food_types[i]\n",
    "                food_color = (255, 105, 180) if food_type == 'pink' else (0, 128, 0)\n",
    "                pygame.draw.circle(self.screen, food_color, (int(food_position[0]), int(food_position[1])),\n",
    "                                   self.food_radius)\n",
    "\n",
    "            pygame.draw.circle(self.screen, (141, 144, 226), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])),\n",
    "                               self.blue_dot_radius)\n",
    "\n",
    "            # Display episode number and elapsed time\n",
    "            episode_text = f\"Episode: {self.episode_number}\"\n",
    "            time_text = f\"Elapsed Time: {int(self.elapsed_time)}s\"\n",
    "            # reward_text = f\"Rewards: {self.total_reward}\"  # Display total reward\n",
    "\n",
    "            episode_surface = self.font.render(episode_text, True, (255, 189, 221))\n",
    "            time_surface = self.font.render(time_text, True, (255, 189, 221))\n",
    "            # reward_surface = self.font.render(reward_text, True, (255, 189, 221))\n",
    "\n",
    "            self.screen.blit(episode_surface, (10, 10))\n",
    "            self.screen.blit(time_surface, (10, 50))\n",
    "            # self.screen.blit(reward_surface, (10, 90))  # Display the total reward\n",
    "\n",
    "            pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DotEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Models', 'DQN_Model')\n",
    "log_path = os.path.join('Training', 'DQN_Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env=env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Shanila\\CSE\\cse465\\New folder\\CSE_465\\dqn\\dqn2\\updated\\updated.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Shanila/CSE/cse465/New%20folder/CSE_465/dqn/dqn2/updated/updated.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:312\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    309\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    311\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 312\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollect_rollouts(\n\u001b[0;32m    313\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv,\n\u001b[0;32m    314\u001b[0m         train_freq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_freq,\n\u001b[0;32m    315\u001b[0m         action_noise\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_noise,\n\u001b[0;32m    316\u001b[0m         callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    317\u001b[0m         learning_starts\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_starts,\n\u001b[0;32m    318\u001b[0m         replay_buffer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplay_buffer,\n\u001b[0;32m    319\u001b[0m         log_interval\u001b[39m=\u001b[39mlog_interval,\n\u001b[0;32m    320\u001b[0m     )\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:544\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    541\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    543\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    547\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs[env_idx]\u001b[39m.\u001b[39mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\nehli\\anaconda3\\envs\\myenv1\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} Std reward: {std_reward}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
