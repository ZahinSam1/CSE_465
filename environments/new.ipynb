{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotEnv(gym.Env):\n",
    "    def __init__(self, screen_width=400, screen_height=400, render_mode='human'):\n",
    "        super(DotEnv, self).__init__()\n",
    "\n",
    "        # screen dimensions\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # defining the agent policies\n",
    "        self.direction_line_length = 20\n",
    "        self.game_speed = 1\n",
    "        self.test_health = 10\n",
    "\n",
    "        # for Blue dot\n",
    "        self.blue_dot_radius = 20\n",
    "        self.blue_dot_health = 5\n",
    "        self.blue_dot_attack_dmg = 1\n",
    "        self.blue_dot_search_radius = 100\n",
    "        self.blue_dot_turn_rate = None\n",
    "        self.blue_dot_stamina = 50\n",
    "        self.blue_dot_stamina_recovery_rate = 15\n",
    "\n",
    "        # for Red dot\n",
    "        self.red_dot_radius = self.blue_dot_radius - 10\n",
    "        self.red_dot_health = 50\n",
    "        self.red_dot_attack_dmg = 1\n",
    "        self.red_dot_search_radius = None\n",
    "        self.red_dot_turn_rate = None\n",
    "\n",
    "        # action space (left, right, up, down) for both\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Define observation space (positions of blue dot and red dot)\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0], dtype=np.float32),\n",
    "                                            high=np.array([self.screen_width / 2, self.screen_height, self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        # Initializing the pygame window\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Dots Moving Environment')\n",
    "\n",
    "        # Initializing the positions of the blue and red dots\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        # Define grid line properties\n",
    "        self.grid_color = (0, 0, 0)\n",
    "        self.grid_spacing = 20  # Adjust this value to change the grid spacing\n",
    "\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "\n",
    "        # defining the reward for the agent\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.obstacles = [\n",
    "            (200, 100, 20, 150),\n",
    "            (300, 250, 50, 20)\n",
    "        ]\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset the positions of the blue and red dots\n",
    "        self.blue_dot_pos = np.array([0, 0], dtype=np.float32)\n",
    "        self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "\n",
    "        self.blue_dot_health = self.test_health\n",
    "        self.total_reward = 0\n",
    "\n",
    "\n",
    "        # Return the initial observation (concatenate blue dot position and red dot position)\n",
    "        observation = np.concatenate([self.blue_dot_pos, self.red_dot_pos])\n",
    "        # print(observation)\n",
    "\n",
    "        return [observation, seed]\n",
    "\n",
    "\n",
    "    def collides_with_obstacle(self, position, obstacle):\n",
    "        # Check if the dot's position collides with the given obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "        return (x <= dot_x <= x + width) and (y <= dot_y <= y + height)\n",
    "\n",
    "\n",
    "    def resolve_collision(self, position, obstacle):\n",
    "        # Adjust the dot's position to resolve the collision with the obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "\n",
    "        # Calculate the nearest valid position outside the obstacle\n",
    "        new_x = max(x, min(dot_x, x + width))\n",
    "        new_y = max(y, min(dot_y, y + height))\n",
    "\n",
    "        position[0] = new_x\n",
    "        position[1] = new_y\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # Define the movement speed\n",
    "        move_speed = 0.9 * self.game_speed\n",
    "        offset = 3\n",
    "        # unpacking the action for blue and red dots\n",
    "        action_blue_dot = action\n",
    "        # print(self.red_dot_pos.dtype, self.red_dot_pos.dtype)\n",
    "\n",
    "        if action_blue_dot == 0:  # Move blue dot left\n",
    "            self.blue_dot_pos[0] -= move_speed\n",
    "            # reward -= 0.01\n",
    "\n",
    "        elif action_blue_dot == 1:  # Move blue dot right\n",
    "            self.blue_dot_pos[0] += move_speed\n",
    "            # reward -= 0.01\n",
    "\n",
    "        elif action_blue_dot == 2:  # Move blue dot up\n",
    "            self.blue_dot_pos[1] -= move_speed\n",
    "            # reward -= 0.01\n",
    "\n",
    "        elif action_blue_dot == 3:  # Move blue dot down\n",
    "            self.blue_dot_pos[1] += move_speed\n",
    "            # reward -= 0.01\n",
    "        reward += 0.001\n",
    "        move_speed = 0.05 * self.game_speed\n",
    "\n",
    "        # Calculate the direction vector from the red dot to the blue dot\n",
    "        direction = self.blue_dot_pos - self.red_dot_pos\n",
    "\n",
    "        # Normalize the direction vector\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        # print(\"Direction: \", direction)\n",
    "        distance_between_centers = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "\n",
    "\n",
    "\n",
    "        # Clip red dot position to stay within the entire screen\n",
    "        self.red_dot_pos = np.clip(self.red_dot_pos, [0, 0], [self.screen_width, self.screen_height])\n",
    "\n",
    "        # Check if the dots collide with each other\n",
    "        if distance_between_centers < self.blue_dot_radius + self.red_dot_radius:\n",
    "            # Separate the dots by moving the red dot away from the blue dot\n",
    "            self.red_dot_pos -= -50.0 + move_speed * direction\n",
    "            self.blue_dot_health -= self.red_dot_attack_dmg\n",
    "            if(self.blue_dot_health <= 0):\n",
    "                done = True\n",
    "            reward -= 5\n",
    "            # print(\"collision\")\n",
    "\n",
    "        else:\n",
    "            # Move the red dot towards the blue dot with a fixed speed\n",
    "            self.red_dot_pos += move_speed * direction\n",
    "\n",
    "        # Clip blue dot position to stay within the first half of the screen\n",
    "        self.blue_dot_pos[0] = np.clip(self.blue_dot_pos[0], 0, self.screen_width)\n",
    "        self.blue_dot_pos[1] = np.clip(self.blue_dot_pos[1], 0, self.screen_height)\n",
    "\n",
    "\n",
    "        # Define a simple reward function (e.g., distance between the two dots)\n",
    "        # reward = -np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "        # # check for the dots' collision with the obstacles\n",
    "        # for obstacle in self.obstacles:\n",
    "        #     if self.collides_with_obstacle(self.blue_dot_pos, obstacle):\n",
    "        #         self.resolve_collision(self.blue_dot_pos, obstacle)\n",
    "        #     if self.collides_with_obstacle(self.red_dot_pos, obstacle):\n",
    "        #         self.resolve_collision(self.red_dot_pos, obstacle)\n",
    "\n",
    "        # Check if the dots are close to each other (you can adjust the distance threshold as needed)\n",
    "        # done = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos) < 10\n",
    "\n",
    "        # print(self.red_dot_pos.dtype, self.red_dot_pos.dtype)\n",
    "\n",
    "        observation = np.concatenate([self.blue_dot_pos, self.red_dot_pos])\n",
    "\n",
    "        return observation, reward, done, False, {}\n",
    "\n",
    "    def display_total_reward(self):\n",
    "        text_surface = self.font.render(f\"Reward: {self.total_reward: .2f} Blue Health: {self.blue_dot_health}\", True, (0, 0, 0))\n",
    "\n",
    "        text_rect = text_surface.get_rect()\n",
    "\n",
    "        text_rect.center = (self.screen_width - 200, 10)\n",
    "\n",
    "        self.screen.blit(text_surface, text_rect)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            # Clear the screen\n",
    "            self.screen.fill((255, 255, 255))\n",
    "\n",
    "            # Draw grid lines\n",
    "            for x in range(0, self.screen_width, self.grid_spacing):\n",
    "                pygame.draw.line(self.screen, self.grid_color, (x, 0), (x, self.screen_height), 1)\n",
    "            for y in range(0, self.screen_height, self.grid_spacing):\n",
    "                pygame.draw.line(self.screen, self.grid_color, (0, y), (self.screen_width, y), 1)\n",
    "\n",
    "            # Draw blue dot\n",
    "            pygame.draw.circle(self.screen, (0, 0, 255), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])), self.blue_dot_radius)\n",
    "\n",
    "            # draw the blue dot search radius\n",
    "            pygame.draw.circle(self.screen, (0, 0, 255), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])),\n",
    "                               self.blue_dot_search_radius, 1)\n",
    "\n",
    "            # Draw red dot\n",
    "            pygame.draw.circle(self.screen, (255, 0, 0), (int(self.red_dot_pos[0]), int(self.red_dot_pos[1])), self.red_dot_radius)\n",
    "\n",
    "            # # calculating the position of facing direction lines\n",
    "            # blue_dot_direction_end = tuple(map(int, self.blue_dot_pos + self.direction_line_length * action_blue))\n",
    "            # red_dot_direction_end = tuple(map(int, self.red_dot_pos + self.direction_line_length * action_red))\n",
    "            #\n",
    "            # # direction line draw\n",
    "            # pygame.draw.line(self.screen, (0, 0, 255), tuple(map(int, self.blue_dot_pos)), blue_dot_direction_end, 2)\n",
    "            # pygame.draw.line(self.screen, (255, 0, 0), tuple(map(int, self.red_dot_pos)), red_dot_direction_end, 2)\n",
    "\n",
    "            self.display_total_reward()\n",
    "\n",
    "            # Update the display\n",
    "            pygame.display.update()\n",
    "\n",
    "            # # Draw rectangular obstacles\n",
    "            # for obstacle in self.obstacles:\n",
    "            #     pygame.draw.rect(self.screen, (128, 128, 128), obstacle)\n",
    "\n",
    "    def collides_with_obstacle(self, position, obstacle):\n",
    "        # Check if the dot's position collides with the given obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "        return (x <= dot_x <= x + width) and (y <= dot_y <= y + height)\n",
    "\n",
    "    def resolve_collision(self, position, obstacle):\n",
    "        # Adjust the dot's position to resolve the collision with the obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "\n",
    "        # Calculate the nearest valid position outside the obstacle\n",
    "        new_x = max(x, min(dot_x, x + width))\n",
    "        new_y = max(y, min(dot_y, y + height))\n",
    "\n",
    "        position[0] = new_x\n",
    "        position[1] = new_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DotEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Models', 'DQN_Model')\n",
    "log_path = os.path.join('Training', 'DQN_Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env=env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.65e+04 |\n",
      "|    ep_rew_mean      | -33.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1756     |\n",
      "|    time_elapsed     | 37       |\n",
      "|    total_timesteps  | 65897    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000271 |\n",
      "|    n_updates        | 303974   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73e+04 |\n",
      "|    ep_rew_mean      | -32.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 871      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total_timesteps  | 138665   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000122 |\n",
      "|    n_updates        | 322166   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61e+04 |\n",
      "|    ep_rew_mean      | -33.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 759      |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 193253   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.69e-05 |\n",
      "|    n_updates        | 335813   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74e+04 |\n",
      "|    ep_rew_mean      | -32.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 699      |\n",
      "|    time_elapsed     | 398      |\n",
      "|    total_timesteps  | 278933   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.58e-05 |\n",
      "|    n_updates        | 357233   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69e+04 |\n",
      "|    ep_rew_mean      | -33.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 683      |\n",
      "|    time_elapsed     | 495      |\n",
      "|    total_timesteps  | 338454   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000203 |\n",
      "|    n_updates        | 372113   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.67e+04 |\n",
      "|    ep_rew_mean      | -33.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 675      |\n",
      "|    time_elapsed     | 593      |\n",
      "|    total_timesteps  | 401395   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000178 |\n",
      "|    n_updates        | 387848   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74e+04 |\n",
      "|    ep_rew_mean      | -32.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 680      |\n",
      "|    time_elapsed     | 714      |\n",
      "|    total_timesteps  | 486239   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000134 |\n",
      "|    n_updates        | 409059   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x18276af9490>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_5\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+04 |\n",
      "|    ep_rew_mean      | -28.2    |\n",
      "|    exploration_rate | 0.17     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1481     |\n",
      "|    time_elapsed     | 58       |\n",
      "|    total_timesteps  | 87326    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.85e-05 |\n",
      "|    n_updates        | 421831   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.39e+04 |\n",
      "|    ep_rew_mean      | -26.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 900      |\n",
      "|    time_elapsed     | 212      |\n",
      "|    total_timesteps  | 190887   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000156 |\n",
      "|    n_updates        | 447721   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.75e+04 |\n",
      "|    ep_rew_mean      | -22.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 748      |\n",
      "|    time_elapsed     | 440      |\n",
      "|    total_timesteps  | 329580   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.56e-05 |\n",
      "|    n_updates        | 482394   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.42e+04 |\n",
      "|    ep_rew_mean      | -25.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 738      |\n",
      "|    time_elapsed     | 525      |\n",
      "|    total_timesteps  | 387932   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.91e-05 |\n",
      "|    n_updates        | 496982   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+04 |\n",
      "|    ep_rew_mean      | -25.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 728      |\n",
      "|    time_elapsed     | 663      |\n",
      "|    total_timesteps  | 482915   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 520728   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.29e+04 |\n",
      "|    ep_rew_mean      | -27.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 706      |\n",
      "|    time_elapsed     | 777      |\n",
      "|    total_timesteps  | 549080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000176 |\n",
      "|    n_updates        | 537269   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.16e+04 |\n",
      "|    ep_rew_mean      | -28.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 706      |\n",
      "|    time_elapsed     | 856      |\n",
      "|    total_timesteps  | 605357   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00012  |\n",
      "|    n_updates        | 551339   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.09e+04 |\n",
      "|    ep_rew_mean      | -29.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 705      |\n",
      "|    time_elapsed     | 949      |\n",
      "|    total_timesteps  | 669591   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.98e-06 |\n",
      "|    n_updates        | 567397   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.04e+04 |\n",
      "|    ep_rew_mean      | -29.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 704      |\n",
      "|    time_elapsed     | 1044     |\n",
      "|    total_timesteps  | 735638   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.31e-06 |\n",
      "|    n_updates        | 583909   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.08e+04 |\n",
      "|    ep_rew_mean      | -29.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 708      |\n",
      "|    time_elapsed     | 1174     |\n",
      "|    total_timesteps  | 832876   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.81e-06 |\n",
      "|    n_updates        | 608218   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.15e+04 |\n",
      "|    ep_rew_mean      | -28.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 713      |\n",
      "|    time_elapsed     | 1323     |\n",
      "|    total_timesteps  | 944148   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.98e-05 |\n",
      "|    n_updates        | 636036   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x18276af9490>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_6\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+04 |\n",
      "|    ep_rew_mean      | -25.9    |\n",
      "|    exploration_rate | 0.085    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1454     |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total_timesteps  | 96311    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.51e-06 |\n",
      "|    n_updates        | 661577   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.1e+04  |\n",
      "|    ep_rew_mean      | -28.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 1008     |\n",
      "|    time_elapsed     | 166      |\n",
      "|    total_timesteps  | 168400   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.36e-05 |\n",
      "|    n_updates        | 679599   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+04 |\n",
      "|    ep_rew_mean      | -27.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 931      |\n",
      "|    time_elapsed     | 288      |\n",
      "|    total_timesteps  | 268990   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000127 |\n",
      "|    n_updates        | 704747   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.33e+04 |\n",
      "|    ep_rew_mean      | -26.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 417      |\n",
      "|    total_timesteps  | 373112   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.37e-05 |\n",
      "|    n_updates        | 730777   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.38e+04 |\n",
      "|    ep_rew_mean      | -26.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 897      |\n",
      "|    time_elapsed     | 531      |\n",
      "|    total_timesteps  | 476518   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.32e-06 |\n",
      "|    n_updates        | 756629   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.31e+04 |\n",
      "|    ep_rew_mean      | -26.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 903      |\n",
      "|    time_elapsed     | 613      |\n",
      "|    total_timesteps  | 554227   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.62e-05 |\n",
      "|    n_updates        | 776056   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.38e+04 |\n",
      "|    ep_rew_mean      | -26.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 911      |\n",
      "|    time_elapsed     | 731      |\n",
      "|    total_timesteps  | 666418   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.26e-05 |\n",
      "|    n_updates        | 804104   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.35e+04 |\n",
      "|    ep_rew_mean      | -26.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 910      |\n",
      "|    time_elapsed     | 826      |\n",
      "|    total_timesteps  | 752555   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.32e-06 |\n",
      "|    n_updates        | 825638   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.38e+04 |\n",
      "|    ep_rew_mean      | -26.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 913      |\n",
      "|    time_elapsed     | 938      |\n",
      "|    total_timesteps  | 857112   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.92e-05 |\n",
      "|    n_updates        | 851777   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.34e+04 |\n",
      "|    ep_rew_mean      | -26.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 908      |\n",
      "|    time_elapsed     | 1029     |\n",
      "|    total_timesteps  | 934759   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.66e-05 |\n",
      "|    n_updates        | 871189   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x18276af9490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN_Logs\\DQN_9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.83e+04 |\n",
      "|    ep_rew_mean      | -31.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1354     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total_timesteps  | 73287    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.51e-06 |\n",
      "|    n_updates        | 1018321  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.16e+04 |\n",
      "|    ep_rew_mean      | -28.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 872      |\n",
      "|    time_elapsed     | 198      |\n",
      "|    total_timesteps  | 173011   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.93e-06 |\n",
      "|    n_updates        | 1043252  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+04 |\n",
      "|    ep_rew_mean      | -28.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 823      |\n",
      "|    time_elapsed     | 307      |\n",
      "|    total_timesteps  | 253237   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.84e-06 |\n",
      "|    n_updates        | 1063309  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2e+04    |\n",
      "|    ep_rew_mean      | -30      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 804      |\n",
      "|    time_elapsed     | 398      |\n",
      "|    total_timesteps  | 320631   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4e-06    |\n",
      "|    n_updates        | 1080157  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.11e+04 |\n",
      "|    ep_rew_mean      | -28.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 748      |\n",
      "|    time_elapsed     | 563      |\n",
      "|    total_timesteps  | 421767   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.23e-06 |\n",
      "|    n_updates        | 1105441  |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x18275c2cdd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} Std reward: {std_reward}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
