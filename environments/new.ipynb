{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotEnv(gym.Env):\n",
    "    def __init__(self, screen_width=400, screen_height=400, render_mode='human'):\n",
    "        super(DotEnv, self).__init__()\n",
    "\n",
    "        # screen dimensions\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # defining the agent policies\n",
    "        self.direction_line_length = 20\n",
    "        self.game_speed = 1\n",
    "        self.test_health = 10\n",
    "\n",
    "        # for Blue dot\n",
    "        self.blue_dot_radius = 20\n",
    "        self.blue_dot_health = 5\n",
    "        self.blue_dot_attack_dmg = 1\n",
    "        self.blue_dot_search_radius = 100\n",
    "        self.blue_dot_turn_rate = None\n",
    "        self.blue_dot_stamina = 50\n",
    "        self.blue_dot_stamina_recovery_rate = 15\n",
    "\n",
    "        # for Red dot\n",
    "        self.red_dot_radius = self.blue_dot_radius - 10\n",
    "        self.red_dot_health = 50\n",
    "        self.red_dot_attack_dmg = 1\n",
    "        self.red_dot_search_radius = None\n",
    "        self.red_dot_turn_rate = None\n",
    "\n",
    "        # action space (left, right, up, down) for both\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Define observation space (positions of blue dot and red dot)\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0], dtype=np.float32),\n",
    "                                            high=np.array([self.screen_width / 2, self.screen_height, self.screen_width, self.screen_height], dtype=np.float32),\n",
    "                                            dtype=np.float32)\n",
    "\n",
    "        # Initializing the pygame window\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "        pygame.display.set_caption('Dots Moving Environment')\n",
    "\n",
    "        # Initializing the positions of the blue and red dots\n",
    "        self.blue_dot_pos = np.array([self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "        self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "        # Define grid line properties\n",
    "        self.grid_color = (0, 0, 0)\n",
    "        self.grid_spacing = 20  # Adjust this value to change the grid spacing\n",
    "\n",
    "        pygame.font.init()\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "\n",
    "        # defining the reward for the agent\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.obstacles = [\n",
    "            (200, 100, 20, 150),\n",
    "            (300, 250, 50, 20)\n",
    "        ]\n",
    "\n",
    "    def reset(self, seed=0):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset the positions of the blue and red dots\n",
    "        self.blue_dot_pos = np.array([0, 0], dtype=np.float32)\n",
    "        self.red_dot_pos = np.array([3 * self.screen_width / 4, self.screen_height / 2], dtype=np.float32)\n",
    "\n",
    "\n",
    "        self.blue_dot_health = self.test_health\n",
    "        self.total_reward = 0\n",
    "\n",
    "\n",
    "        # Return the initial observation (concatenate blue dot position and red dot position)\n",
    "        observation = np.concatenate([self.blue_dot_pos, self.red_dot_pos])\n",
    "        # print(observation)\n",
    "\n",
    "        return [observation, seed]\n",
    "\n",
    "\n",
    "    def collides_with_obstacle(self, position, obstacle):\n",
    "        # Check if the dot's position collides with the given obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "        return (x <= dot_x <= x + width) and (y <= dot_y <= y + height)\n",
    "\n",
    "\n",
    "    def resolve_collision(self, position, obstacle):\n",
    "        # Adjust the dot's position to resolve the collision with the obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "\n",
    "        # Calculate the nearest valid position outside the obstacle\n",
    "        new_x = max(x, min(dot_x, x + width))\n",
    "        new_y = max(y, min(dot_y, y + height))\n",
    "\n",
    "        position[0] = new_x\n",
    "        position[1] = new_y\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        done = False\n",
    "        # Define the movement speed\n",
    "        move_speed = 0.9 * self.game_speed\n",
    "        offset = 3\n",
    "        # unpacking the action for blue and red dots\n",
    "        action_blue_dot = action\n",
    "        # print(self.red_dot_pos.dtype, self.red_dot_pos.dtype)\n",
    "\n",
    "        if action_blue_dot == 0:  # Move blue dot left\n",
    "            self.blue_dot_pos[0] -= move_speed\n",
    "            # reward -= 0.01\n",
    "\n",
    "        elif action_blue_dot == 1:  # Move blue dot right\n",
    "            self.blue_dot_pos[0] += move_speed\n",
    "            # reward -= 0.01\n",
    "\n",
    "        elif action_blue_dot == 2:  # Move blue dot up\n",
    "            self.blue_dot_pos[1] -= move_speed\n",
    "            # reward -= 0.01\n",
    "\n",
    "        elif action_blue_dot == 3:  # Move blue dot down\n",
    "            self.blue_dot_pos[1] += move_speed\n",
    "            # reward -= 0.01\n",
    "        reward += 0.001\n",
    "        move_speed = 0.05 * self.game_speed\n",
    "\n",
    "        # Calculate the direction vector from the red dot to the blue dot\n",
    "        direction = self.blue_dot_pos - self.red_dot_pos\n",
    "\n",
    "        # Normalize the direction vector\n",
    "        direction /= np.linalg.norm(direction)\n",
    "        # print(\"Direction: \", direction)\n",
    "        distance_between_centers = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "\n",
    "\n",
    "\n",
    "        # Clip red dot position to stay within the entire screen\n",
    "        self.red_dot_pos = np.clip(self.red_dot_pos, [0, 0], [self.screen_width, self.screen_height])\n",
    "\n",
    "        # Check if the dots collide with each other\n",
    "        if distance_between_centers < self.blue_dot_radius + self.red_dot_radius:\n",
    "            # Separate the dots by moving the red dot away from the blue dot\n",
    "            self.red_dot_pos -= -50.0 + move_speed * direction\n",
    "            self.blue_dot_health -= self.red_dot_attack_dmg\n",
    "            if(self.blue_dot_health <= 0):\n",
    "                done = True\n",
    "            reward -= 5\n",
    "            # print(\"collision\")\n",
    "\n",
    "        else:\n",
    "            # Move the red dot towards the blue dot with a fixed speed\n",
    "            self.red_dot_pos += move_speed * direction\n",
    "\n",
    "        # Clip blue dot position to stay within the first half of the screen\n",
    "        self.blue_dot_pos[0] = np.clip(self.blue_dot_pos[0], 0, self.screen_width)\n",
    "        self.blue_dot_pos[1] = np.clip(self.blue_dot_pos[1], 0, self.screen_height)\n",
    "\n",
    "\n",
    "        # Define a simple reward function (e.g., distance between the two dots)\n",
    "        # reward = -np.linalg.norm(self.blue_dot_pos - self.red_dot_pos)\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "        # # check for the dots' collision with the obstacles\n",
    "        # for obstacle in self.obstacles:\n",
    "        #     if self.collides_with_obstacle(self.blue_dot_pos, obstacle):\n",
    "        #         self.resolve_collision(self.blue_dot_pos, obstacle)\n",
    "        #     if self.collides_with_obstacle(self.red_dot_pos, obstacle):\n",
    "        #         self.resolve_collision(self.red_dot_pos, obstacle)\n",
    "\n",
    "        # Check if the dots are close to each other (you can adjust the distance threshold as needed)\n",
    "        # done = np.linalg.norm(self.blue_dot_pos - self.red_dot_pos) < 10\n",
    "\n",
    "        # print(self.red_dot_pos.dtype, self.red_dot_pos.dtype)\n",
    "\n",
    "        observation = np.concatenate([self.blue_dot_pos, self.red_dot_pos])\n",
    "\n",
    "        return observation, reward, done, False, {}\n",
    "\n",
    "    def display_total_reward(self):\n",
    "        text_surface = self.font.render(f\"Reward: {self.total_reward: .2f} Blue Health: {self.blue_dot_health}\", True, (0, 0, 0))\n",
    "\n",
    "        text_rect = text_surface.get_rect()\n",
    "\n",
    "        text_rect.center = (self.screen_width - 200, 10)\n",
    "\n",
    "        self.screen.blit(text_surface, text_rect)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == 'human':\n",
    "            # Clear the screen\n",
    "            self.screen.fill((255, 255, 255))\n",
    "\n",
    "            # Draw grid lines\n",
    "            for x in range(0, self.screen_width, self.grid_spacing):\n",
    "                pygame.draw.line(self.screen, self.grid_color, (x, 0), (x, self.screen_height), 1)\n",
    "            for y in range(0, self.screen_height, self.grid_spacing):\n",
    "                pygame.draw.line(self.screen, self.grid_color, (0, y), (self.screen_width, y), 1)\n",
    "\n",
    "            # Draw blue dot\n",
    "            pygame.draw.circle(self.screen, (0, 0, 255), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])), self.blue_dot_radius)\n",
    "\n",
    "            # draw the blue dot search radius\n",
    "            pygame.draw.circle(self.screen, (0, 0, 255), (int(self.blue_dot_pos[0]), int(self.blue_dot_pos[1])),\n",
    "                               self.blue_dot_search_radius, 1)\n",
    "\n",
    "            # Draw red dot\n",
    "            pygame.draw.circle(self.screen, (255, 0, 0), (int(self.red_dot_pos[0]), int(self.red_dot_pos[1])), self.red_dot_radius)\n",
    "\n",
    "            # # calculating the position of facing direction lines\n",
    "            # blue_dot_direction_end = tuple(map(int, self.blue_dot_pos + self.direction_line_length * action_blue))\n",
    "            # red_dot_direction_end = tuple(map(int, self.red_dot_pos + self.direction_line_length * action_red))\n",
    "            #\n",
    "            # # direction line draw\n",
    "            # pygame.draw.line(self.screen, (0, 0, 255), tuple(map(int, self.blue_dot_pos)), blue_dot_direction_end, 2)\n",
    "            # pygame.draw.line(self.screen, (255, 0, 0), tuple(map(int, self.red_dot_pos)), red_dot_direction_end, 2)\n",
    "\n",
    "            self.display_total_reward()\n",
    "\n",
    "            # Update the display\n",
    "            pygame.display.update()\n",
    "\n",
    "            # # Draw rectangular obstacles\n",
    "            # for obstacle in self.obstacles:\n",
    "            #     pygame.draw.rect(self.screen, (128, 128, 128), obstacle)\n",
    "\n",
    "    def collides_with_obstacle(self, position, obstacle):\n",
    "        # Check if the dot's position collides with the given obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "        return (x <= dot_x <= x + width) and (y <= dot_y <= y + height)\n",
    "\n",
    "    def resolve_collision(self, position, obstacle):\n",
    "        # Adjust the dot's position to resolve the collision with the obstacle\n",
    "        x, y, width, height = obstacle\n",
    "        dot_x, dot_y = position\n",
    "\n",
    "        # Calculate the nearest valid position outside the obstacle\n",
    "        new_x = max(x, min(dot_x, x + width))\n",
    "        new_y = max(y, min(dot_y, y + height))\n",
    "\n",
    "        position[0] = new_x\n",
    "        position[1] = new_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DotEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Models', 'DQN_Model')\n",
    "log_path = os.path.join('Training', 'DQN_Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env=env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=500000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DQN.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} Std reward: {std_reward}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
